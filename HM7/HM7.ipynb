{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import gc\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchtnt.utils.data import CudaDataPrefetcher\n",
    "\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_499596/2313373535.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean = torch.load('data/mean.pt')\n",
      "/tmp/ipykernel_499596/2313373535.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  std = torch.load('data/std.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor([0.4914, 0.4822, 0.4465])\n",
      "Std:  tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "dl = False\n",
    "data_path = './data'\n",
    "\n",
    "# Load CIFAR-10 dataset with the simple transform\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transforms.ToTensor())\n",
    "try: \n",
    "    mean = torch.load('data/mean.pt')\n",
    "    std = torch.load('data/std.pt')\n",
    "except FileNotFoundError:\n",
    "    print(\"Computing Mean and Std\")\n",
    "    train_imgs = torch.stack([img for img, _ in cifar10_train], dim=3)#.to(device=device)\n",
    "    view = train_imgs.view(3, -1)#.to(device=device)\n",
    "\n",
    "    mean = train_imgs.view(3, -1).mean(dim=1)\n",
    "    std = train_imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    torch.save(mean, 'data/mean.pt')\n",
    "    torch.save(std, 'data/std.pt')\n",
    "\n",
    "# Define the transform with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    @classmethod\n",
    "    def compare_results(cls, results1, results2):\n",
    "        print('Comparing results:')\n",
    "        comparisons = {\n",
    "            'accuracy': 100*(results1['accuracy'] - results2['accuracy'])/results1['accuracy'],\n",
    "            'precision': 100*(results1['precision'] - results2['precision'])/results1['precision'],\n",
    "            'recall': 100*(results1['recall'] - results2['recall'])/results1['recall'],\n",
    "            'f1': 100*(results1['f1'] - results2['f1'])/results1['f1']\n",
    "        }\n",
    "        for key, value in comparisons.items():\n",
    "            print(f'{key}: {value} %')\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            return self.forward(x).argmax(dim=1)\n",
    "    \n",
    "    def train_model(\n",
    "        self,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        train_len,\n",
    "        test_len,\n",
    "        test_size,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.SGD,\n",
    "        optimizer_args = [],\n",
    "        optimizer_kwargs = {},\n",
    "        print_epoch=10,\n",
    "        header_epoch = 15\n",
    "    ):  \n",
    "        scaler = GradScaler(\"cuda\")\n",
    "        optimizer = optimizer(self.parameters(), *optimizer_args, **optimizer_kwargs)\n",
    "        training_time = 0\n",
    "        train_hist = np.zeros(epochs)\n",
    "        test_hist = np.zeros(epochs)\n",
    "        accuracy_hist = np.zeros(epochs)\n",
    "        \n",
    "        cell_width = 20\n",
    "        header_form_spec = f'^{cell_width}'\n",
    "        \n",
    "        epoch_inspection = {\n",
    "            \"Epoch\": 0,\n",
    "            \"Epoch Time (s)\": 0,\n",
    "            \"Training Loss\": 0,\n",
    "            \"Test Loss \": 0,\n",
    "            \"Overfit (%)\": 0,\n",
    "            \"Accuracy (%)\": 0,\n",
    "            \"Δ Accuracy (%)\": 0,\n",
    "            \"GPU Memory (GiB)\": 0\n",
    "        }\n",
    "\n",
    "        header_string = \"|\"\n",
    "        for key in epoch_inspection.keys():\n",
    "            header_string += (f\"{key:{header_form_spec}}|\")\n",
    "        \n",
    "        divider_string = '-'*len(header_string)\n",
    "        if print_epoch:\n",
    "            print(f'Training {self.__class__.__name__}\\n')\n",
    "            print(divider_string)\n",
    "        max_accuracy = 0            \n",
    "        for epoch in range(epochs):\n",
    "            begin_epoch = time.time()\n",
    "            self.train()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            train_loss = 0\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                #X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast(\"cuda\"):\n",
    "                    Y_pred = self.forward(X_batch)\n",
    "                    loss = loss_fn(Y_pred, Y_batch)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.detach()\n",
    "            training_time += time.time() - start_time\n",
    "            \n",
    "            train_loss = train_loss/train_len\n",
    "            train_hist[epoch] = train_loss\n",
    "            \n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = 0\n",
    "                correct = 0               \n",
    "                \n",
    "                for X_test_batch, Y_test_batch in test_loader:\n",
    "                    #X_test_batch, Y_test_batch = X_test_batch.to(device, non_blocking=True), Y_test_batch.to(device, non_blocking=True)\n",
    "                    \n",
    "                    out = self.forward(X_test_batch)\n",
    "                    test_loss += loss_fn(out, Y_test_batch).detach()\n",
    "                    correct += (out.argmax(dim=1) == Y_test_batch).sum()\n",
    "                    \n",
    "            test_loss = test_loss/test_len\n",
    "            test_hist[epoch] = test_loss\n",
    "            accuracy = correct/test_size\n",
    "            accuracy_hist[epoch] = accuracy\n",
    "            end_epoch = time.time()\n",
    "            if print_epoch and (epoch % print_epoch == 0 or epoch == epochs - 1) :\n",
    "                mem = (torch.cuda.memory_allocated() + torch.cuda.memory_reserved())/1024**3\n",
    "                if header_epoch and epoch % header_epoch == 0:\n",
    "                    print(header_string)\n",
    "                    print(divider_string)\n",
    "                epoch_duration = end_epoch - begin_epoch\n",
    "                overfit = 100 * (test_loss - train_loss) / train_loss\n",
    "                d_accuracy = 0 if max_accuracy == 0 else 100 * (accuracy - max_accuracy) / max_accuracy\n",
    "                if accuracy > max_accuracy:\n",
    "                    max_accuracy = accuracy\n",
    "                \n",
    "                epoch_inspection['Epoch'] = f'{epoch}'\n",
    "                epoch_inspection['Epoch Time (s)'] = f'{epoch_duration:4f}'\n",
    "                epoch_inspection['Training Loss'] = f'{train_loss:8f}'\n",
    "                epoch_inspection['Test Loss '] = f'{test_loss:8f}'\n",
    "                epoch_inspection['Overfit (%)'] = f'{overfit:4f}'\n",
    "                epoch_inspection['Accuracy (%)'] = f'{accuracy:4f}'\n",
    "                epoch_inspection['Δ Accuracy (%)'] = f'{d_accuracy:4f}'\n",
    "                epoch_inspection[\"GPU Memory (GiB)\"] = f'{mem:2f}'\n",
    "                for value in epoch_inspection.values():\n",
    "                    print(f\"|{value:^{cell_width}}\", end='')\n",
    "                print('|')\n",
    "                print(divider_string)\n",
    "            \n",
    "\n",
    "        print(f'\\nTraining Time: {training_time} seconds\\n')\n",
    "        \n",
    "        self.train_hist = train_hist\n",
    "        self.test_hist = test_hist\n",
    "        self.accuracy_hist = accuracy_hist\n",
    "    \n",
    "    def plot_training(self, title='Training Results'):\n",
    "        plt.plot(self.train_hist, label='Training Loss')\n",
    "        plt.plot(self.test_hist, label='Test Loss')\n",
    "        plt.plot(self.accuracy_hist, label='Accuracy')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_results(self, Y_test=None, Y_pred=None):\n",
    "        if Y_test is None:\n",
    "            Y_test = self.last_test\n",
    "        if Y_pred is None:\n",
    "            Y_pred = self.last_pred\n",
    "            \n",
    "        if isinstance(Y_test, torch.Tensor):\n",
    "            Y_test = Y_test.cpu().detach().numpy()\n",
    "        if isinstance(Y_pred, torch.Tensor):\n",
    "            Y_pred = Y_pred.cpu().detach().numpy()\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(Y_test, Y_pred),\n",
    "            'precision': precision_score(Y_test, Y_pred, average='weighted'),\n",
    "            'recall': recall_score(Y_test, Y_pred, average='weighted'),\n",
    "            'f1': f1_score(Y_test, Y_pred, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(Y_test, Y_pred),\n",
    "            'classification_report': classification_report(Y_test, Y_pred)\n",
    "        }\n",
    "        self.last_results = results\n",
    "        return results\n",
    "    def print_results(self, results=None):\n",
    "        if results is None:\n",
    "            try: \n",
    "                results = self.last_results\n",
    "            except:\n",
    "                results = self.get_results()\n",
    "        for key, value in results.items():\n",
    "            if key in ['confusion_matrix', 'classification_report']:\n",
    "                print(f'{key.capitalize()}:\\n{value}')\n",
    "            else:\n",
    "                print(f'{key.capitalize()}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvImageClassifier(Classifier):\n",
    "    def __init__(self, input_dim, conv_layers, fc_layers, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stack = nn.Sequential(OrderedDict(\n",
    "            [\n",
    "                ('conv0', nn.Conv2d(in_channels=3, out_channels=conv_layers[0], kernel_size=3, padding=1)),\n",
    "                ('activation0', activation()),\n",
    "                ('maxpool0', nn.MaxPool2d(2)),\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "        for i in range(1, len(conv_layers)):\n",
    "            self.stack.add_module(f'conv{i}', nn.Conv2d(in_channels=conv_layers[i-1], out_channels=conv_layers[i], kernel_size=3, padding=1))\n",
    "            self.stack.add_module(f'activation{i}', activation())\n",
    "            self.stack.add_module(f'maxpool{i}', nn.MaxPool2d(2))\n",
    "            \n",
    "        conv_out = input_dim//(2**len(conv_layers))\n",
    "        self.stack.add_module('flatten', nn.Flatten())\n",
    "        self.stack.add_module(f'fc0', nn.Linear(conv_out**2*conv_layers[-1], fc_layers[0]))\n",
    "        \n",
    "        for i in range(1, len(fc_layers)):\n",
    "            self.stack.add_module(f'activation_fc{i}', nn.Tanh())\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(fc_layers[i-1], fc_layers[i]))        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "workers = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1a = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1a.stack)\n",
    "\n",
    "model_1a.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    train_len=len(cifar10_train),\n",
    "    test_loader=test_loader,\n",
    "    test_len=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 8e-3, 'weight_decay': 1e-2},\n",
    "    print_epoch=1,\n",
    "    header_epoch=15\n",
    ")\n",
    "\n",
    "del train_loader\n",
    "del test_loader\n",
    "del cifar10_train\n",
    "del cifar10_test\n",
    "\n",
    "model_1a.plot_training(\"2 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "workers = 16\n",
    "\n",
    "# Deal with loaders sticking around after interrupting training\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1b = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64, 128],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1b.stack)\n",
    "\n",
    "model_1b.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 3e-4, 'weight_decay': 1e-2}, #Increase alpha to 2 next time\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    print_epoch=1\n",
    ")\n",
    "del train_loader\n",
    "del test_loader\n",
    "model_1b.plot_training(\"3 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after last bn but before last weight\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, nonlinearity = 'relu', stride=1, dropout = 0.4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.conv2 = nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.shortcut = nn.Conv2d(in_chans, out_chans, kernel_size=1, stride=1, bias=False) if in_chans != out_chans else nn.Identity()\n",
    "        \n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "    def forward(self, x):\n",
    "        out = self.batch_norm1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        return out\n",
    "class ResNet(Classifier):\n",
    "    def __init__(self, input_dim = 32, n_blocks = 10, conv_channels = [32,16], fc_channels = [32, 10], dropout_p=0.4, dropout_h=0.4, nonlinearity='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add initial convolutions\n",
    "        self.h1 = nn.Sequential()\n",
    "        for i in range(len(conv_channels)):\n",
    "            self.h1.add_module(\n",
    "                name=f'conv{i}',\n",
    "                module=nn.Conv2d(\n",
    "                    in_channels = 3 if i == 0 else conv_channels[i-1],\n",
    "                    out_channels=conv_channels[i],\n",
    "                    kernel_size=3,\n",
    "                    padding=1\n",
    "                )\n",
    "            )\n",
    "            self.h1.add_module(\n",
    "                name=f'activation{i}',\n",
    "                module=nn.ReLU()\n",
    "            )\n",
    "        self.h1.add_module(\n",
    "            name=f'maxpool',\n",
    "            module=nn.MaxPool2d(2)\n",
    "        )\n",
    "        #output of h1 before maxpool is 16 32x32 images. After maxpool, 16 16x16 images\n",
    "# h1_in: torch.Size([1024, 3, 32, 32])\n",
    "# res_block_in: torch.Size([1024, 16, 16, 16])\n",
    "# h2_in: torch.Size([1024, 64])\n",
    "        #Add Resblocks\n",
    "        res_block_in = conv_channels[0]//2\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[\n",
    "                ResBlock(\n",
    "                    in_chans=conv_channels[-1],\n",
    "                    out_chans=conv_channels[-1],\n",
    "                    nonlinearity=nonlinearity,\n",
    "                    dropout=dropout_p\n",
    "                ) for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "        #output of resblocks is 16 16x16 images\n",
    "        \n",
    "        # Add final layers\n",
    "        self.h2 = nn.Sequential()\n",
    "        self.h2.add_module(\n",
    "            name='final_batch_norm',\n",
    "            module=nn.BatchNorm2d(\n",
    "                num_features=conv_channels[-1]\n",
    "            )\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name='final_relu',\n",
    "            module=nn.ReLU()\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'dropout_head',\n",
    "            module=nn.Dropout(dropout_h)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'gap',\n",
    "            module=nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'flatten',\n",
    "            module=nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #output is 16 8x8 images\n",
    "        #   16 comes from conv_channels[-1]\n",
    "        #   8x8 comes from input_dim // 4\n",
    "        fc_in = conv_channels[-1] * (input_dim//4)**2\n",
    "        \n",
    "        for i in range(len(fc_channels)):\n",
    "            \n",
    "            self.h2.add_module(\n",
    "                name=f'fc{i}',\n",
    "                module=nn.Linear(\n",
    "                    in_features=fc_in if i == 0 else fc_channels[i-1],\n",
    "                    out_features=fc_channels[i]\n",
    "                )\n",
    "            )\n",
    "            if i < len(fc_channels) - 1:\n",
    "                self.h2.add_module(\n",
    "                    name = f'fc_activation{i}',\n",
    "                    module=nn.ReLU()\n",
    "                )\n",
    "        self.h2.add_module('softmax', nn.Softmax(dim=1))\n",
    "    def forward(self, x):\n",
    "        #print(f\"h1_in: {x.shape}\")\n",
    "        out = self.h1(x)\n",
    "        #print(f\"res_block_in: {out.shape}\")\n",
    "        out = self.resblocks(out)\n",
    "        #print(f\"h2_in: {out.shape}\")\n",
    "        out = self.h2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin init train_loader\n",
      "Batch Size: 12.0 MiB\n",
      "begin init fetcher\n",
      "Init time: 3.40 seconds\n",
      "Training ResNet\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         0          |      7.116063      |      2.265676      |      2.274211      |      0.376715      |      0.187600      |      0.000000      |      3.130038      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         1          |      5.478538      |      2.212338      |      2.206912      |     -0.245290      |      0.271700      |     44.829418      |      3.600742      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         2          |      5.408134      |      2.157184      |      2.154635      |     -0.118138      |      0.306000      |     12.624217      |      4.071446      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         3          |      5.401286      |      2.102207      |      2.084073      |     -0.862634      |      0.378000      |     23.529411      |      4.542149      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         4          |      5.392175      |      2.055118      |      2.058239      |      0.151837      |      0.403400      |      6.719581      |      5.014805      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         5          |      5.407612      |      2.020173      |      2.003168      |     -0.841793      |      0.459500      |     13.906788      |      5.485508      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         6          |      5.453823      |      1.998484      |      1.988662      |     -0.491497      |      0.474000      |      3.155602      |      5.956212      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         7          |      5.412908      |      1.984436      |      1.967690      |     -0.843851      |      0.493900      |      4.198318      |      6.426915      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         8          |      5.455698      |      1.974415      |      1.967549      |     -0.347747      |      0.492500      |     -0.283463      |      6.897618      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         9          |      5.418330      |      1.963472      |      1.946472      |     -0.865781      |      0.516300      |      4.535323      |      7.368321      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         10         |      5.447134      |      1.954979      |      1.941486      |     -0.690147      |      0.520400      |      0.794117      |      7.839024      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         11         |      5.452911      |      1.942589      |      1.931226      |     -0.584900      |      0.531200      |      2.075327      |      8.309727      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         12         |      5.530963      |      1.932980      |      1.925115      |     -0.406913      |      0.537300      |      1.148343      |      8.780430      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         13         |      5.388495      |      1.923811      |      1.904878      |     -0.984149      |      0.555300      |      3.350085      |      9.251133      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         14         |      5.427806      |      1.913800      |      1.914241      |      0.023022      |      0.547200      |     -1.458677      |      9.721837      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         15         |      5.432960      |      1.907161      |      1.899160      |     -0.419535      |      0.562400      |      1.278586      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         16         |      5.407106      |      1.900558      |      1.896206      |     -0.229028      |      0.565900      |      0.622330      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         17         |      5.417626      |      1.889570      |      1.886853      |     -0.143790      |      0.574600      |      1.537377      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         18         |      5.432478      |      1.887240      |      1.873487      |     -0.728733      |      0.588600      |      2.436477      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         19         |      5.413687      |      1.880052      |      1.869369      |     -0.568226      |      0.589800      |      0.203877      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         20         |      5.418052      |      1.874712      |      1.856139      |     -0.990677      |      0.608400      |      3.153609      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         21         |      5.466951      |      1.868374      |      1.867770      |     -0.032342      |      0.590900      |     -2.876395      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         22         |      5.474858      |      1.864380      |      1.873511      |      0.489771      |      0.587500      |     -3.435242      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         23         |      5.390116      |      1.860441      |      1.844232      |     -0.871252      |      0.618400      |      1.643654      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         24         |      5.479653      |      1.858649      |      1.849040      |     -0.516994      |      0.613600      |     -0.776200      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         25         |      5.427269      |      1.852556      |      1.836089      |     -0.888892      |      0.628300      |      1.600911      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         26         |      5.608012      |      1.849153      |      1.839090      |     -0.544179      |      0.624900      |     -0.541147      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         27         |      5.553706      |      1.846410      |      1.859567      |      0.712572      |      0.604200      |     -3.835748      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         28         |      5.512042      |      1.843247      |      1.828875      |     -0.779711      |      0.636800      |      1.352854      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         29         |      5.520800      |      1.841669      |      1.838703      |     -0.161052      |      0.622900      |     -2.182786      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         30         |      5.517484      |      1.837863      |      1.826913      |     -0.595773      |      0.635200      |     -0.251261      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         31         |      5.520252      |      1.836700      |      1.833901      |     -0.152401      |      0.631000      |     -0.910805      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         32         |      5.481586      |      1.833218      |      1.838506      |      0.288494      |      0.620200      |     -2.606786      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         33         |      5.533232      |      1.827356      |      1.819949      |     -0.405370      |      0.644900      |      1.271981      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         34         |      5.473633      |      1.826698      |      1.828157      |      0.079838      |      0.633700      |     -1.736696      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         35         |      5.539631      |      1.825461      |      1.813034      |     -0.680758      |      0.647400      |      0.387657      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         36         |      5.368767      |      1.821318      |      1.834630      |      0.730918      |      0.626500      |     -3.228291      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         37         |      5.441316      |      1.820512      |      1.859541      |      2.143847      |      0.599000      |     -7.476056      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         38         |      5.397823      |      1.816635      |      1.817319      |      0.037614      |      0.645900      |     -0.231688      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         39         |      5.409083      |      1.815647      |      1.804467      |     -0.615761      |      0.657700      |      1.590986      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         40         |      5.433081      |      1.811068      |      1.823552      |      0.689328      |      0.641700      |     -2.432725      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         41         |      5.391478      |      1.809553      |      1.801628      |     -0.437975      |      0.662000      |      0.653793      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         42         |      5.468803      |      1.810588      |      1.809120      |     -0.081115      |      0.653300      |     -1.314201      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         43         |      5.727251      |      1.810449      |      1.800094      |     -0.571997      |      0.663600      |      0.241687      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         44         |      5.566763      |      1.806268      |      1.801369      |     -0.271197      |      0.661900      |     -0.256176      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         45         |      5.557188      |      1.804095      |      1.795417      |     -0.480982      |      0.666000      |      0.361670      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         46         |      5.518191      |      1.803452      |      1.794186      |     -0.513780      |      0.665900      |     -0.015018      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         47         |      5.406463      |      1.802639      |      1.792906      |     -0.539942      |      0.669000      |      0.450445      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         48         |      5.472936      |      1.799983      |      1.794374      |     -0.311656      |      0.667300      |     -0.254108      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         49         |      5.440919      |      1.799880      |      1.796309      |     -0.198417      |      0.666700      |     -0.343791      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         50         |      5.455334      |      1.798012      |      1.795956      |     -0.114395      |      0.666400      |     -0.388642      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         51         |      5.441148      |      1.795948      |      1.785059      |     -0.606312      |      0.675600      |      0.986550      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         52         |      5.434140      |      1.793104      |      1.791111      |     -0.111151      |      0.671700      |     -0.577263      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         53         |      5.441097      |      1.791979      |      1.785239      |     -0.376152      |      0.675600      |      0.000000      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         54         |      5.459394      |      1.790749      |      1.803032      |      0.685912      |      0.662200      |     -1.983425      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         55         |      5.439835      |      1.788289      |      1.778646      |     -0.539235      |      0.686300      |      1.583776      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         56         |      5.446946      |      1.788064      |      1.795524      |      0.417231      |      0.668100      |     -2.651899      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         57         |      5.503527      |      1.785677      |      1.781001      |     -0.261854      |      0.679600      |     -0.976246      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         58         |      5.435332      |      1.785917      |      1.780061      |     -0.327887      |      0.682800      |     -0.509979      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         59         |      5.455494      |      1.785573      |      1.785283      |     -0.016250      |      0.675700      |     -1.544510      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         60         |      5.478451      |      1.785062      |      1.772502      |     -0.703651      |      0.690500      |      0.611975      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         61         |      5.469033      |      1.780100      |      1.775587      |     -0.253533      |      0.687000      |     -0.506877      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         62         |      5.441483      |      1.782677      |      1.776862      |     -0.326183      |      0.687100      |     -0.492392      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         63         |      5.444178      |      1.779595      |      1.767374      |     -0.686715      |      0.696300      |      0.839972      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         64         |      5.440589      |      1.778158      |      1.767359      |     -0.607310      |      0.693400      |     -0.416488      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         65         |      5.505507      |      1.777386      |      1.767517      |     -0.555232      |      0.693800      |     -0.359040      |     10.192540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "    del resnet\n",
    "    del train_loader_cuda\n",
    "    del test_loader_cuda\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reset CUDA context\n",
    "start = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "batch_size = int(2**11)\n",
    "workers = 12\n",
    "cpu_prefetch = 39\n",
    "gpu_prefetch = 28\n",
    "\n",
    "print('begin init train_loader')\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=cpu_prefetch,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "X_batch = next(iter(train_loader))[0]\n",
    "dtype_size = X_batch.element_size()\n",
    "print(f\"Batch Size: {X_batch.element_size() * X_batch.nelement() / 1024**2} MiB\")\n",
    "\n",
    "\n",
    "print('begin init fetcher')\n",
    "train_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = train_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=gpu_prefetch\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=1)\n",
    "test_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = test_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=1\n",
    ")\n",
    "\n",
    "resnet = ResNet(\n",
    "    input_dim = 32,\n",
    "    conv_channels=[16,8],\n",
    "    n_blocks = 10,\n",
    "    fc_channels=[16,10],\n",
    "    dropout_h = 0.6,\n",
    "    dropout_p = 0.4\n",
    ").to(device=device)\n",
    "print(f\"Init time: {(time.time() - start):.2f} seconds\")\n",
    "resnet.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader_cuda,\n",
    "    train_len=len(train_loader),\n",
    "    test_loader=test_loader_cuda,\n",
    "    test_len=len(test_loader),\n",
    "    test_size=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 5e-3, 'weight_decay': 1e-3},\n",
    "    print_epoch=1\n",
    ")\n",
    "resnet.plot_training(\"ResNet Training Curves\")\n",
    "\n",
    "#[m1,n1]x[m2,n2] is only possible if n1 == m2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
