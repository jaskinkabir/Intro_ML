{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import datetime\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = False\n",
    "data_path = './data'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=dl, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_imgs = torch.stack([img for img, _ in cifar10], dim=3)#.to(device=device)\n",
    "view = train_imgs.view(3, -1)#.to(device=device)\n",
    "\n",
    "mean = train_imgs.view(3, -1).mean(dim=1)\n",
    "std = train_imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 50000\n",
      "test size 10000\n"
     ]
    }
   ],
   "source": [
    "# Batch size of 64 surprisingly gave the best results\n",
    "workers = 16\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "# Load all images in one batch\n",
    "test_loader = DataLoader(cifar10_test, batch_size=64, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "print(f'training size {len(cifar10_train)}')\n",
    "print(f'test size {len(cifar10_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    @classmethod\n",
    "    def compare_results(cls, results1, results2):\n",
    "        print('Comparing results:')\n",
    "        comparisons = {\n",
    "            'accuracy': 100*(results1['accuracy'] - results2['accuracy'])/results1['accuracy'],\n",
    "            'precision': 100*(results1['precision'] - results2['precision'])/results1['precision'],\n",
    "            'recall': 100*(results1['recall'] - results2['recall'])/results1['recall'],\n",
    "            'f1': 100*(results1['f1'] - results2['f1'])/results1['f1']\n",
    "        }\n",
    "        for key, value in comparisons.items():\n",
    "            print(f'{key}: {value} %')\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_results(self, Y_test=None, Y_pred=None):\n",
    "        if Y_test is None:\n",
    "            Y_test = self.last_test\n",
    "        if Y_pred is None:\n",
    "            Y_pred = self.last_pred\n",
    "            \n",
    "        if isinstance(Y_test, torch.Tensor):\n",
    "            Y_test = Y_test.cpu().detach().numpy()\n",
    "        if isinstance(Y_pred, torch.Tensor):\n",
    "            Y_pred = Y_pred.cpu().detach().numpy()\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(Y_test, Y_pred),\n",
    "            'precision': precision_score(Y_test, Y_pred, average='weighted'),\n",
    "            'recall': recall_score(Y_test, Y_pred, average='weighted'),\n",
    "            'f1': f1_score(Y_test, Y_pred, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(Y_test, Y_pred),\n",
    "            'classification_report': classification_report(Y_test, Y_pred)\n",
    "        }\n",
    "        self.last_results = results\n",
    "        return results\n",
    "    def print_results(self, results=None):\n",
    "        if results is None:\n",
    "            try: \n",
    "                results = self.last_results\n",
    "            except:\n",
    "                results = self.get_results()\n",
    "        for key, value in results.items():\n",
    "            if key in ['confusion_matrix', 'classification_report']:\n",
    "                print(f'{key.capitalize()}:\\n{value}')\n",
    "            else:\n",
    "                print(f'{key.capitalize()}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvImageClassifier(Classifier):\n",
    "    def __init__(self, input_dim, conv_layers, fc_layers, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stack = nn.Sequential(OrderedDict(\n",
    "            [\n",
    "                ('conv0', nn.Conv2d(in_channels=3, out_channels=conv_layers[0], kernel_size=3, padding=1)),\n",
    "                ('activation0', activation()),\n",
    "                ('maxpool0', nn.MaxPool2d(2)),\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "        for i in range(1, len(conv_layers)):\n",
    "            self.stack.add_module(f'conv{i}', nn.Conv2d(in_channels=conv_layers[i-1], out_channels=conv_layers[i], kernel_size=3, padding=1))\n",
    "            self.stack.add_module(f'activation{i}', activation())\n",
    "            self.stack.add_module(f'maxpool{i}', nn.MaxPool2d(2))\n",
    "            \n",
    "        conv_out = input_dim//(2**len(conv_layers))\n",
    "        self.stack.add_module('flatten', nn.Flatten())\n",
    "        self.stack.add_module(f'fc0', nn.Linear(conv_out**2*conv_layers[-1], fc_layers[0]))\n",
    "        \n",
    "        for i in range(1, len(fc_layers)):\n",
    "            self.stack.add_module(f'activation_fc{i}', activation())\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(fc_layers[i-1], fc_layers[i]))        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x).argmax(dim=1)\n",
    "    def train_model(\n",
    "        self,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        alpha,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.SGD,\n",
    "        print_epoch=10,\n",
    "    ):  \n",
    "        optimizer = optimizer(self.parameters(), lr=alpha)\n",
    "        training_time = 0\n",
    "        train_hist = np.zeros(epochs)\n",
    "        test_hist = np.zeros(epochs)\n",
    "        accuracy_hist = np.zeros(epochs)\n",
    "        for epoch in range(epochs):\n",
    "            begin_epoch = time.time()\n",
    "            self.train()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            train_loss = 0\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                Y_pred = self.forward(X_batch)\n",
    "                loss = loss_fn(Y_pred, Y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            training_time += time.time() - start_time\n",
    "            train_hist[epoch] = train_loss/len(train_loader)\n",
    "            \n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = 0\n",
    "                Y_pred_eval = []\n",
    "                Y_test = []\n",
    "                for X_test_batch, Y_test_batch in test_loader:\n",
    "                    X_test_batch, Y_test_batch = X_test_batch.to(device, non_blocking=True), Y_test_batch.to(device, non_blocking=True)\n",
    "                    \n",
    "                    out = self.forward(X_test_batch)\n",
    "                    test_loss += loss_fn(out, Y_test_batch).detach()\n",
    "                    Y_test.extend(Y_test_batch.cpu().detach().numpy())\n",
    "                    Y_pred_eval.extend(out.argmax(dim=1).cpu().detach().numpy())\n",
    "                \n",
    "            test_hist[epoch] = test_loss/len(test_loader)\n",
    "            accuracy_hist[epoch] = accuracy_score(Y_test, Y_pred_eval)\n",
    "            if epoch % print_epoch == 0 or epoch == epochs - 1:\n",
    "                end_epoch = time.time()\n",
    "                print(f'Epoch {epoch}: Training Loss: {train_hist[epoch]}, Test Loss: {test_hist[epoch]}, Accuracy: {accuracy_hist[epoch]}, Epoch Duration: {(end_epoch - begin_epoch):2f} seconds')\n",
    "        self.last_pred = torch.tensor(Y_pred_eval)\n",
    "        self.last_test = torch.tensor(Y_test)\n",
    "        print(f'\\nTraining Time: {training_time} seconds\\n')\n",
    "        \n",
    "        self.train_hist = train_hist\n",
    "        self.test_hist = test_hist\n",
    "        self.accuracy_hist = accuracy_hist\n",
    "    \n",
    "    def plot_training(self, title='Training Results'):\n",
    "        plt.plot(self.train_hist, label='Training Loss')\n",
    "        plt.plot(self.test_hist, label='Test Loss')\n",
    "        plt.plot(self.accuracy_hist, label='Accuracy')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activation0): ReLU()\n",
      "  (maxpool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activation1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc0): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (activation_fc1): ReLU()\n",
      "  (fc1): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 0: Training Loss: 2.036158863379031, Test Loss: 1.805673599243164, Accuracy: 0.3684, Epoch Duration: 2.926650 seconds\n",
      "Epoch 1: Training Loss: 1.7084384718719794, Test Loss: 1.6134552955627441, Accuracy: 0.426, Epoch Duration: 2.717023 seconds\n",
      "Epoch 2: Training Loss: 1.559805147501887, Test Loss: 1.4922003746032715, Accuracy: 0.472, Epoch Duration: 2.922448 seconds\n",
      "Epoch 3: Training Loss: 1.4715076514652796, Test Loss: 1.4391804933547974, Accuracy: 0.4839, Epoch Duration: 2.867933 seconds\n",
      "Epoch 4: Training Loss: 1.4139337247731734, Test Loss: 1.38300621509552, Accuracy: 0.506, Epoch Duration: 2.856355 seconds\n",
      "Epoch 5: Training Loss: 1.3705287977140777, Test Loss: 1.3547425270080566, Accuracy: 0.5185, Epoch Duration: 2.551744 seconds\n",
      "Epoch 6: Training Loss: 1.336233411516462, Test Loss: 1.322646975517273, Accuracy: 0.522, Epoch Duration: 2.667912 seconds\n",
      "Epoch 7: Training Loss: 1.3021318425937576, Test Loss: 1.2940900325775146, Accuracy: 0.5376, Epoch Duration: 2.862749 seconds\n",
      "Epoch 8: Training Loss: 1.276149302112813, Test Loss: 1.2746564149856567, Accuracy: 0.5411, Epoch Duration: 2.894225 seconds\n",
      "Epoch 9: Training Loss: 1.2517643802020015, Test Loss: 1.2481803894042969, Accuracy: 0.5559, Epoch Duration: 2.720627 seconds\n",
      "Epoch 10: Training Loss: 1.2353347661543865, Test Loss: 1.2427504062652588, Accuracy: 0.5576, Epoch Duration: 2.624754 seconds\n",
      "Epoch 11: Training Loss: 1.2161709362146806, Test Loss: 1.2434266805648804, Accuracy: 0.5613, Epoch Duration: 2.685026 seconds\n",
      "Epoch 12: Training Loss: 1.2000249429624907, Test Loss: 1.2113131284713745, Accuracy: 0.5694, Epoch Duration: 2.706989 seconds\n",
      "Epoch 13: Training Loss: 1.184628669096499, Test Loss: 1.2075117826461792, Accuracy: 0.5711, Epoch Duration: 2.672885 seconds\n",
      "Epoch 14: Training Loss: 1.1739544041302739, Test Loss: 1.1982440948486328, Accuracy: 0.5765, Epoch Duration: 2.713288 seconds\n",
      "Epoch 15: Training Loss: 1.1608956225064335, Test Loss: 1.1917665004730225, Accuracy: 0.5779, Epoch Duration: 2.770755 seconds\n",
      "Epoch 16: Training Loss: 1.1493183131120643, Test Loss: 1.1799571514129639, Accuracy: 0.5807, Epoch Duration: 2.778637 seconds\n",
      "Epoch 17: Training Loss: 1.1392024439208361, Test Loss: 1.1717298030853271, Accuracy: 0.5829, Epoch Duration: 2.676955 seconds\n",
      "Epoch 18: Training Loss: 1.12859211162645, Test Loss: 1.1577389240264893, Accuracy: 0.591, Epoch Duration: 2.693056 seconds\n",
      "Epoch 19: Training Loss: 1.1103187191243074, Test Loss: 1.1436491012573242, Accuracy: 0.5955, Epoch Duration: 2.696106 seconds\n",
      "Epoch 20: Training Loss: 1.1080911062201675, Test Loss: 1.1434687376022339, Accuracy: 0.594, Epoch Duration: 2.814191 seconds\n",
      "Epoch 21: Training Loss: 1.0935943953844967, Test Loss: 1.1437177658081055, Accuracy: 0.5998, Epoch Duration: 2.782064 seconds\n",
      "Epoch 22: Training Loss: 1.0829489401408605, Test Loss: 1.1245415210723877, Accuracy: 0.6025, Epoch Duration: 2.577798 seconds\n",
      "Epoch 23: Training Loss: 1.0782546267217519, Test Loss: 1.1168800592422485, Accuracy: 0.6036, Epoch Duration: 2.808075 seconds\n",
      "Epoch 24: Training Loss: 1.0643364069413166, Test Loss: 1.1155198812484741, Accuracy: 0.6076, Epoch Duration: 2.676967 seconds\n",
      "Epoch 25: Training Loss: 1.0575000522088032, Test Loss: 1.103281021118164, Accuracy: 0.6142, Epoch Duration: 2.837254 seconds\n",
      "Epoch 26: Training Loss: 1.052183940702555, Test Loss: 1.097813606262207, Accuracy: 0.6148, Epoch Duration: 2.849738 seconds\n",
      "Epoch 27: Training Loss: 1.040924990663723, Test Loss: 1.092650294303894, Accuracy: 0.6158, Epoch Duration: 2.898786 seconds\n",
      "Epoch 28: Training Loss: 1.0421719733549624, Test Loss: 1.0839499235153198, Accuracy: 0.6198, Epoch Duration: 2.772419 seconds\n",
      "Epoch 29: Training Loss: 1.027793243223307, Test Loss: 1.0886030197143555, Accuracy: 0.6151, Epoch Duration: 2.766535 seconds\n",
      "Epoch 30: Training Loss: 1.0218037245224934, Test Loss: 1.0832148790359497, Accuracy: 0.6218, Epoch Duration: 2.718185 seconds\n",
      "Epoch 31: Training Loss: 1.0188454134123666, Test Loss: 1.082734227180481, Accuracy: 0.6186, Epoch Duration: 2.740639 seconds\n",
      "Epoch 32: Training Loss: 1.0158482875142778, Test Loss: 1.067803144454956, Accuracy: 0.6238, Epoch Duration: 2.684464 seconds\n",
      "Epoch 33: Training Loss: 1.0076187143520432, Test Loss: 1.0781785249710083, Accuracy: 0.6179, Epoch Duration: 2.697218 seconds\n",
      "Epoch 34: Training Loss: 0.9978498256936366, Test Loss: 1.0651644468307495, Accuracy: 0.6292, Epoch Duration: 2.782734 seconds\n",
      "Epoch 35: Training Loss: 0.9946390949949926, Test Loss: 1.0628859996795654, Accuracy: 0.625, Epoch Duration: 2.737664 seconds\n",
      "Epoch 36: Training Loss: 0.9894393864943056, Test Loss: 1.0620944499969482, Accuracy: 0.6321, Epoch Duration: 2.664324 seconds\n",
      "Epoch 37: Training Loss: 0.9868008360570791, Test Loss: 1.0581214427947998, Accuracy: 0.6272, Epoch Duration: 2.670968 seconds\n",
      "Epoch 38: Training Loss: 0.9828516232724093, Test Loss: 1.0576820373535156, Accuracy: 0.6353, Epoch Duration: 2.660934 seconds\n",
      "Epoch 39: Training Loss: 0.9816133854340534, Test Loss: 1.0585943460464478, Accuracy: 0.6313, Epoch Duration: 2.558878 seconds\n",
      "Epoch 40: Training Loss: 0.9745710844896278, Test Loss: 1.0453414916992188, Accuracy: 0.6356, Epoch Duration: 2.681855 seconds\n",
      "Epoch 41: Training Loss: 0.9694891790954434, Test Loss: 1.0488041639328003, Accuracy: 0.6306, Epoch Duration: 2.755283 seconds\n",
      "Epoch 42: Training Loss: 0.9675847936649712, Test Loss: 1.039642333984375, Accuracy: 0.6352, Epoch Duration: 2.669586 seconds\n",
      "Epoch 43: Training Loss: 0.9635105424997757, Test Loss: 1.0410791635513306, Accuracy: 0.6323, Epoch Duration: 2.663557 seconds\n",
      "Epoch 44: Training Loss: 0.9595387176591523, Test Loss: 1.0424931049346924, Accuracy: 0.6365, Epoch Duration: 2.668001 seconds\n",
      "Epoch 45: Training Loss: 0.9545841411668428, Test Loss: 1.0386461019515991, Accuracy: 0.6329, Epoch Duration: 2.723891 seconds\n",
      "Epoch 46: Training Loss: 0.952189028263092, Test Loss: 1.0424283742904663, Accuracy: 0.6341, Epoch Duration: 2.705916 seconds\n",
      "Epoch 47: Training Loss: 0.9492500485206137, Test Loss: 1.0327448844909668, Accuracy: 0.6351, Epoch Duration: 2.712341 seconds\n",
      "Epoch 48: Training Loss: 0.9445843574952106, Test Loss: 1.0327484607696533, Accuracy: 0.6369, Epoch Duration: 2.739713 seconds\n",
      "Epoch 49: Training Loss: 0.9436409826181373, Test Loss: 1.027201771736145, Accuracy: 0.6428, Epoch Duration: 2.730023 seconds\n",
      "Epoch 50: Training Loss: 0.9382607815216999, Test Loss: 1.0302848815917969, Accuracy: 0.6383, Epoch Duration: 2.630923 seconds\n",
      "Epoch 51: Training Loss: 0.9377691770086483, Test Loss: 1.0312974452972412, Accuracy: 0.6393, Epoch Duration: 2.616817 seconds\n",
      "Epoch 52: Training Loss: 0.9374745865257419, Test Loss: 1.0310231447219849, Accuracy: 0.6403, Epoch Duration: 2.867736 seconds\n",
      "Epoch 53: Training Loss: 0.9357191470204568, Test Loss: 1.0270494222640991, Accuracy: 0.6434, Epoch Duration: 2.799059 seconds\n",
      "Epoch 54: Training Loss: 0.9360508480850531, Test Loss: 1.0264078378677368, Accuracy: 0.6426, Epoch Duration: 2.685614 seconds\n",
      "Epoch 55: Training Loss: 0.928781373160226, Test Loss: 1.0280014276504517, Accuracy: 0.6416, Epoch Duration: 2.525848 seconds\n",
      "Epoch 56: Training Loss: 0.924178807102904, Test Loss: 1.0231996774673462, Accuracy: 0.6428, Epoch Duration: 2.542121 seconds\n",
      "Epoch 57: Training Loss: 0.9189738071694666, Test Loss: 1.03052818775177, Accuracy: 0.6388, Epoch Duration: 2.783607 seconds\n",
      "Epoch 58: Training Loss: 0.9192288943699428, Test Loss: 1.0135607719421387, Accuracy: 0.6441, Epoch Duration: 2.724336 seconds\n",
      "Epoch 59: Training Loss: 0.9143840828720404, Test Loss: 1.015075445175171, Accuracy: 0.6487, Epoch Duration: 2.639720 seconds\n",
      "Epoch 60: Training Loss: 0.9114571201558016, Test Loss: 1.0203649997711182, Accuracy: 0.6418, Epoch Duration: 2.621368 seconds\n",
      "Epoch 61: Training Loss: 0.9117400767851849, Test Loss: 1.0184534788131714, Accuracy: 0.6456, Epoch Duration: 2.808766 seconds\n",
      "Epoch 62: Training Loss: 0.9101453508649554, Test Loss: 1.0182989835739136, Accuracy: 0.6437, Epoch Duration: 2.787845 seconds\n",
      "Epoch 63: Training Loss: 0.9084827960753927, Test Loss: 1.0116134881973267, Accuracy: 0.6482, Epoch Duration: 6.036292 seconds\n",
      "Epoch 64: Training Loss: 0.9105938393242505, Test Loss: 1.0311145782470703, Accuracy: 0.6384, Epoch Duration: 4.570000 seconds\n",
      "Epoch 65: Training Loss: 0.9042842899050031, Test Loss: 1.0087859630584717, Accuracy: 0.6479, Epoch Duration: 4.016515 seconds\n",
      "Epoch 66: Training Loss: 0.9019622012060515, Test Loss: 1.0252691507339478, Accuracy: 0.6423, Epoch Duration: 5.076872 seconds\n",
      "Epoch 67: Training Loss: 0.9012151749766603, Test Loss: 1.0120511054992676, Accuracy: 0.6453, Epoch Duration: 5.395496 seconds\n",
      "Epoch 68: Training Loss: 0.9011272252822409, Test Loss: 1.017264485359192, Accuracy: 0.6487, Epoch Duration: 5.606418 seconds\n",
      "Epoch 69: Training Loss: 0.8988701883627443, Test Loss: 1.012634515762329, Accuracy: 0.6492, Epoch Duration: 4.231141 seconds\n",
      "Epoch 70: Training Loss: 0.894500161920275, Test Loss: 1.008617877960205, Accuracy: 0.6526, Epoch Duration: 3.968545 seconds\n",
      "Epoch 71: Training Loss: 0.89123279342846, Test Loss: 1.0086021423339844, Accuracy: 0.6487, Epoch Duration: 4.139109 seconds\n",
      "Epoch 72: Training Loss: 0.8902294903385396, Test Loss: 1.0077857971191406, Accuracy: 0.6477, Epoch Duration: 4.582083 seconds\n",
      "Epoch 73: Training Loss: 0.8877172701212824, Test Loss: 1.0027916431427002, Accuracy: 0.6496, Epoch Duration: 4.497828 seconds\n",
      "Epoch 74: Training Loss: 0.8849993627898547, Test Loss: 1.0038888454437256, Accuracy: 0.6539, Epoch Duration: 4.019875 seconds\n",
      "Epoch 75: Training Loss: 0.8852164416897054, Test Loss: 1.0227515697479248, Accuracy: 0.6448, Epoch Duration: 3.504044 seconds\n",
      "Epoch 76: Training Loss: 0.8838580567009595, Test Loss: 1.0058928728103638, Accuracy: 0.6493, Epoch Duration: 3.503906 seconds\n",
      "Epoch 77: Training Loss: 0.8821765780448914, Test Loss: 1.0059258937835693, Accuracy: 0.6489, Epoch Duration: 3.485968 seconds\n",
      "Epoch 78: Training Loss: 0.8833074849479052, Test Loss: 1.0117619037628174, Accuracy: 0.648, Epoch Duration: 3.157835 seconds\n",
      "Epoch 79: Training Loss: 0.8794391580990383, Test Loss: 1.0078197717666626, Accuracy: 0.6501, Epoch Duration: 3.241291 seconds\n",
      "Epoch 80: Training Loss: 0.8748336568170664, Test Loss: 1.0011845827102661, Accuracy: 0.6528, Epoch Duration: 3.282074 seconds\n",
      "Epoch 81: Training Loss: 0.8717852611931003, Test Loss: 1.0135761499404907, Accuracy: 0.6497, Epoch Duration: 3.287015 seconds\n",
      "Epoch 82: Training Loss: 0.872380222593035, Test Loss: 0.9998078942298889, Accuracy: 0.6529, Epoch Duration: 2.907953 seconds\n",
      "Epoch 83: Training Loss: 0.8709852354867118, Test Loss: 1.0046743154525757, Accuracy: 0.6499, Epoch Duration: 2.832569 seconds\n",
      "Epoch 84: Training Loss: 0.8713410727831782, Test Loss: 0.9995217323303223, Accuracy: 0.6517, Epoch Duration: 2.849702 seconds\n",
      "Epoch 85: Training Loss: 0.8674018030263939, Test Loss: 1.0038416385650635, Accuracy: 0.6508, Epoch Duration: 3.318118 seconds\n",
      "Epoch 86: Training Loss: 0.8653366626525412, Test Loss: 1.003299355506897, Accuracy: 0.6532, Epoch Duration: 3.550535 seconds\n",
      "Epoch 87: Training Loss: 0.8657579604460268, Test Loss: 1.0029890537261963, Accuracy: 0.6506, Epoch Duration: 2.957963 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m model_1a \u001b[38;5;241m=\u001b[39m ConvImageClassifier(\n\u001b[1;32m      2\u001b[0m     input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      3\u001b[0m     conv_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m      4\u001b[0m     fc_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m      5\u001b[0m     activation\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU\n\u001b[1;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_1a\u001b[38;5;241m.\u001b[39mstack)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel_1a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m model_1a\u001b[38;5;241m.\u001b[39mget_results()\n\u001b[1;32m     21\u001b[0m model_1a\u001b[38;5;241m.\u001b[39mprint_results()\n",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m, in \u001b[0;36mConvImageClassifier.train_model\u001b[0;34m(self, epochs, train_loader, test_loader, alpha, loss_fn, optimizer, print_epoch)\u001b[0m\n\u001b[1;32m     55\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[1;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(Y_pred, Y_batch)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     60\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_1a = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[16, 8],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1a.stack)\n",
    "\n",
    "model_1a.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    alpha=1e-3,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    print_epoch=1\n",
    ")\n",
    "\n",
    "model_1a.get_results()\n",
    "model_1a.print_results()\n",
    "model_1a.plot_training(\"2 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activation0): ReLU()\n",
      "  (maxpool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activation1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc0): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (activation_fc1): ReLU()\n",
      "  (fc1): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 0.8791261120718352, Test Loss: 1.0047351121902466, Accuracy: 0.6539, Epoch Duration: 3.084385 seconds\n",
      "Epoch 1: Training Loss: 0.8790079294418802, Test Loss: 1.0044175386428833, Accuracy: 0.6536, Epoch Duration: 2.741097 seconds\n",
      "Epoch 2: Training Loss: 0.8790595494970983, Test Loss: 1.0054110288619995, Accuracy: 0.6532, Epoch Duration: 2.818763 seconds\n",
      "Epoch 3: Training Loss: 0.8791011206957758, Test Loss: 1.00322687625885, Accuracy: 0.653, Epoch Duration: 2.878485 seconds\n",
      "Epoch 4: Training Loss: 0.8786848491551925, Test Loss: 1.0035338401794434, Accuracy: 0.6533, Epoch Duration: 2.799796 seconds\n",
      "Epoch 5: Training Loss: 0.8785350991755115, Test Loss: 1.0045143365859985, Accuracy: 0.6528, Epoch Duration: 2.849511 seconds\n",
      "Epoch 6: Training Loss: 0.8786300724866439, Test Loss: 1.0064029693603516, Accuracy: 0.6533, Epoch Duration: 2.837314 seconds\n",
      "Epoch 7: Training Loss: 0.8787848158758513, Test Loss: 1.0045053958892822, Accuracy: 0.6538, Epoch Duration: 2.745900 seconds\n",
      "Epoch 8: Training Loss: 0.87875319743643, Test Loss: 1.003534197807312, Accuracy: 0.6538, Epoch Duration: 2.824438 seconds\n",
      "Epoch 9: Training Loss: 0.8786006144114903, Test Loss: 1.003336787223816, Accuracy: 0.6527, Epoch Duration: 2.902532 seconds\n",
      "Epoch 10: Training Loss: 0.8785022497177124, Test Loss: 1.0034945011138916, Accuracy: 0.6527, Epoch Duration: 2.828430 seconds\n",
      "Epoch 11: Training Loss: 0.8784166002760128, Test Loss: 1.0073814392089844, Accuracy: 0.6534, Epoch Duration: 2.834753 seconds\n",
      "Epoch 12: Training Loss: 0.8785107427713822, Test Loss: 1.0037784576416016, Accuracy: 0.6543, Epoch Duration: 2.776026 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fcabbc43e20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 237, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Training Loss: 0.8782890730974625, Test Loss: 1.002672553062439, Accuracy: 0.6524, Epoch Duration: 2.658063 seconds\n",
      "Epoch 14: Training Loss: 0.8782460957157369, Test Loss: 1.0027246475219727, Accuracy: 0.6521, Epoch Duration: 2.761882 seconds\n",
      "Epoch 15: Training Loss: 0.8782481624155628, Test Loss: 1.0009883642196655, Accuracy: 0.6529, Epoch Duration: 2.920649 seconds\n",
      "Epoch 16: Training Loss: 0.8782350250652858, Test Loss: 1.006370186805725, Accuracy: 0.6528, Epoch Duration: 2.849427 seconds\n",
      "Epoch 17: Training Loss: 0.8781948661317631, Test Loss: 1.0020439624786377, Accuracy: 0.6528, Epoch Duration: 2.829332 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fcabbc43e20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 237, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Training Loss: 0.878077219943611, Test Loss: 1.004275918006897, Accuracy: 0.6535, Epoch Duration: 3.030113 seconds\n",
      "Epoch 19: Training Loss: 0.8779937028884888, Test Loss: 1.0054913759231567, Accuracy: 0.6516, Epoch Duration: 2.836216 seconds\n",
      "Epoch 20: Training Loss: 0.8778746541665525, Test Loss: 1.0032696723937988, Accuracy: 0.6537, Epoch Duration: 2.963073 seconds\n",
      "Epoch 21: Training Loss: 0.8780069630973193, Test Loss: 1.0045653581619263, Accuracy: 0.652, Epoch Duration: 2.842553 seconds\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=workers\n",
    ")\n",
    "\n",
    "model_1b = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[16, 8,4],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1a.stack)\n",
    "\n",
    "model_1a.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    alpha=1e-3,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    print_epoch=1\n",
    ")\n",
    "\n",
    "model_1a.get_results()\n",
    "model_1a.print_results()\n",
    "model_1a.plot_training(\"3 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
