{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import gc\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchtnt.utils.data import CudaDataPrefetcher\n",
    "\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_777250/2313373535.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean = torch.load('data/mean.pt')\n",
      "/tmp/ipykernel_777250/2313373535.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  std = torch.load('data/std.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor([0.4914, 0.4822, 0.4465])\n",
      "Std:  tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "dl = False\n",
    "data_path = './data'\n",
    "\n",
    "# Load CIFAR-10 dataset with the simple transform\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transforms.ToTensor())\n",
    "try: \n",
    "    mean = torch.load('data/mean.pt')\n",
    "    std = torch.load('data/std.pt')\n",
    "except FileNotFoundError:\n",
    "    print(\"Computing Mean and Std\")\n",
    "    train_imgs = torch.stack([img for img, _ in cifar10_train], dim=3)#.to(device=device)\n",
    "    view = train_imgs.view(3, -1)#.to(device=device)\n",
    "\n",
    "    mean = train_imgs.view(3, -1).mean(dim=1)\n",
    "    std = train_imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    torch.save(mean, 'data/mean.pt')\n",
    "    torch.save(std, 'data/std.pt')\n",
    "\n",
    "# Define the transform with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    @classmethod\n",
    "    def compare_results(cls, results1, results2):\n",
    "        print('Comparing results:')\n",
    "        comparisons = {\n",
    "            'accuracy': 100*(results1['accuracy'] - results2['accuracy'])/results1['accuracy'],\n",
    "            'precision': 100*(results1['precision'] - results2['precision'])/results1['precision'],\n",
    "            'recall': 100*(results1['recall'] - results2['recall'])/results1['recall'],\n",
    "            'f1': 100*(results1['f1'] - results2['f1'])/results1['f1']\n",
    "        }\n",
    "        for key, value in comparisons.items():\n",
    "            print(f'{key}: {value} %')\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            return self.forward(x).argmax(dim=1)\n",
    "    \n",
    "    def train_model(\n",
    "        self,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        train_len,\n",
    "        test_len,\n",
    "        test_size,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.SGD,\n",
    "        optimizer_args = [],\n",
    "        optimizer_kwargs = {},\n",
    "        print_epoch=10,\n",
    "        header_epoch = 15\n",
    "    ):  \n",
    "        scaler = GradScaler(\"cuda\")\n",
    "        optimizer = optimizer(self.parameters(), *optimizer_args, **optimizer_kwargs)\n",
    "        training_time = 0\n",
    "        train_hist = torch.zeros(epochs, device=device)\n",
    "        test_hist = torch.zeros(epochs, device=device)\n",
    "        accuracy_hist = torch.zeros(epochs, device=device)\n",
    "        \n",
    "        cell_width = 20\n",
    "        header_form_spec = f'^{cell_width}'\n",
    "        \n",
    "        epoch_inspection = {\n",
    "            \"Epoch\": 0,\n",
    "            \"Epoch Time (s)\": 0,\n",
    "            \"Training Loss\": 0,\n",
    "            \"Test Loss \": 0,\n",
    "            \"Overfit (%)\": 0,\n",
    "            \"Accuracy (%)\": 0,\n",
    "            \"Δ Accuracy (%)\": 0,\n",
    "            \"GPU Memory (GiB)\": 0\n",
    "        }\n",
    "\n",
    "        header_string = \"|\"\n",
    "        for key in epoch_inspection.keys():\n",
    "            header_string += (f\"{key:{header_form_spec}}|\")\n",
    "        \n",
    "        divider_string = '-'*len(header_string)\n",
    "        if print_epoch:\n",
    "            print(f'Training {self.__class__.__name__}\\n')\n",
    "            print(divider_string)\n",
    "        max_accuracy = torch.zeros(1, device=device)            \n",
    "        for epoch in range(epochs):\n",
    "            begin_epoch = time.time()\n",
    "            self.train()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            train_loss = 0\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                #X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(\"cuda\"):\n",
    "                    Y_pred = self.forward(X_batch)\n",
    "                    loss = loss_fn(Y_pred, Y_batch)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss\n",
    "            training_time += time.time() - start_time\n",
    "            \n",
    "            train_loss = train_loss/train_len\n",
    "            train_hist[epoch] = train_loss\n",
    "            \n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = torch.zeros(1, device=device)\n",
    "                correct = torch.zeros(1, device=device)               \n",
    "                \n",
    "                for X_test_batch, Y_test_batch in test_loader:\n",
    "                    #X_test_batch, Y_test_batch = X_test_batch.to(device, non_blocking=True), Y_test_batch.to(device, non_blocking=True)\n",
    "                    \n",
    "                    out = self.forward(X_test_batch)\n",
    "                    test_loss += loss_fn(out, Y_test_batch)\n",
    "                    correct += (out.argmax(dim=1) == Y_test_batch).sum()\n",
    "                    \n",
    "            test_loss = test_loss/test_len\n",
    "            test_hist[epoch] = test_loss\n",
    "            accuracy = correct/test_size\n",
    "            accuracy_hist[epoch] = accuracy\n",
    "            end_epoch = time.time()\n",
    "            if print_epoch and (epoch % print_epoch == 0 or epoch == epochs - 1) :\n",
    "                mem = (torch.cuda.memory_allocated() + torch.cuda.memory_reserved())/1024**3\n",
    "                if header_epoch and epoch % header_epoch == 0:\n",
    "                    print(header_string)\n",
    "                    print(divider_string)\n",
    "                epoch_duration = end_epoch - begin_epoch\n",
    "                overfit = 100 * (test_loss - train_loss) / train_loss\n",
    "                d_accuracy = torch.zeros(1) if max_accuracy == 0 else 100 * (accuracy - max_accuracy) / max_accuracy\n",
    "                if accuracy > max_accuracy:\n",
    "                    max_accuracy = accuracy\n",
    "                \n",
    "                epoch_inspection['Epoch'] = f'{epoch}'\n",
    "                epoch_inspection['Epoch Time (s)'] = f'{epoch_duration:4f}'\n",
    "                epoch_inspection['Training Loss'] = f'{train_loss.item():8f}'\n",
    "                epoch_inspection['Test Loss '] = f'{test_loss.item():8f}'\n",
    "                epoch_inspection['Overfit (%)'] = f'{overfit.item():4f}'\n",
    "                epoch_inspection['Accuracy (%)'] = f'{accuracy.item():4f}'\n",
    "                epoch_inspection['Δ Accuracy (%)'] = f'{d_accuracy.item():4f}'\n",
    "                epoch_inspection[\"GPU Memory (GiB)\"] = f'{mem:2f}'\n",
    "                for value in epoch_inspection.values():\n",
    "                    print(f\"|{value:^{cell_width}}\", end='')\n",
    "                print('|')\n",
    "                print(divider_string)\n",
    "            \n",
    "\n",
    "        print(f'\\nTraining Time: {training_time} seconds\\n')\n",
    "        \n",
    "        self.train_hist = train_hist\n",
    "        self.test_hist = test_hist\n",
    "        self.accuracy_hist = accuracy_hist\n",
    "    \n",
    "    def plot_training(self, title='Training Results'):\n",
    "        plt.plot(self.train_hist, label='Training Loss')\n",
    "        plt.plot(self.test_hist, label='Test Loss')\n",
    "        plt.plot(self.accuracy_hist, label='Accuracy')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_results(self, Y_test=None, Y_pred=None):\n",
    "        if Y_test is None:\n",
    "            Y_test = self.last_test\n",
    "        if Y_pred is None:\n",
    "            Y_pred = self.last_pred\n",
    "            \n",
    "        if isinstance(Y_test, torch.Tensor):\n",
    "            Y_test = Y_test.cpu().detach().numpy()\n",
    "        if isinstance(Y_pred, torch.Tensor):\n",
    "            Y_pred = Y_pred.cpu().detach().numpy()\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(Y_test, Y_pred),\n",
    "            'precision': precision_score(Y_test, Y_pred, average='weighted'),\n",
    "            'recall': recall_score(Y_test, Y_pred, average='weighted'),\n",
    "            'f1': f1_score(Y_test, Y_pred, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(Y_test, Y_pred),\n",
    "            'classification_report': classification_report(Y_test, Y_pred)\n",
    "        }\n",
    "        self.last_results = results\n",
    "        return results\n",
    "    def print_results(self, results=None):\n",
    "        if results is None:\n",
    "            try: \n",
    "                results = self.last_results\n",
    "            except:\n",
    "                results = self.get_results()\n",
    "        for key, value in results.items():\n",
    "            if key in ['confusion_matrix', 'classification_report']:\n",
    "                print(f'{key.capitalize()}:\\n{value}')\n",
    "            else:\n",
    "                print(f'{key.capitalize()}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvImageClassifier(Classifier):\n",
    "    def __init__(self, input_dim, conv_layers, fc_layers, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stack = nn.Sequential(OrderedDict(\n",
    "            [\n",
    "                ('conv0', nn.Conv2d(in_channels=3, out_channels=conv_layers[0], kernel_size=3, padding=1)),\n",
    "                ('activation0', activation()),\n",
    "                ('maxpool0', nn.MaxPool2d(2)),\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "        for i in range(1, len(conv_layers)):\n",
    "            self.stack.add_module(f'conv{i}', nn.Conv2d(in_channels=conv_layers[i-1], out_channels=conv_layers[i], kernel_size=3, padding=1))\n",
    "            self.stack.add_module(f'activation{i}', activation())\n",
    "            self.stack.add_module(f'maxpool{i}', nn.MaxPool2d(2))\n",
    "            \n",
    "        conv_out = input_dim//(2**len(conv_layers))\n",
    "        self.stack.add_module('flatten', nn.Flatten())\n",
    "        self.stack.add_module(f'fc0', nn.Linear(conv_out**2*conv_layers[-1], fc_layers[0]))\n",
    "        \n",
    "        for i in range(1, len(fc_layers)):\n",
    "            self.stack.add_module(f'activation_fc{i}', nn.Tanh())\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(fc_layers[i-1], fc_layers[i]))        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "workers = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1a = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1a.stack)\n",
    "\n",
    "model_1a.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    train_len=len(cifar10_train),\n",
    "    test_loader=test_loader,\n",
    "    test_len=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 8e-3, 'weight_decay': 1e-2},\n",
    "    print_epoch=1,\n",
    "    header_epoch=15\n",
    ")\n",
    "\n",
    "del train_loader\n",
    "del test_loader\n",
    "del cifar10_train\n",
    "del cifar10_test\n",
    "\n",
    "model_1a.plot_training(\"2 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "workers = 16\n",
    "\n",
    "# Deal with loaders sticking around after interrupting training\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1b = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64, 128],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1b.stack)\n",
    "\n",
    "model_1b.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 3e-4, 'weight_decay': 1e-2}, #Increase alpha to 2 next time\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    print_epoch=1\n",
    ")\n",
    "del train_loader\n",
    "del test_loader\n",
    "model_1b.plot_training(\"3 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after last bn but before last weight\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, nonlinearity = 'relu', stride=1, dropout = 0.4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.conv2 = nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.shortcut = nn.Conv2d(in_chans, out_chans, kernel_size=1, stride=1, bias=False) if in_chans != out_chans else nn.Identity()\n",
    "        \n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "    def forward(self, x):\n",
    "        out = self.batch_norm1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        return out\n",
    "class ResNet(Classifier):\n",
    "    def __init__(self, input_dim = 32, n_blocks = 10, conv_channels = [32,16], fc_channels = [32, 10], dropout_p=0.4, dropout_h=0.4, nonlinearity='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add initial convolutions\n",
    "        self.h1 = nn.Sequential()\n",
    "        for i in range(len(conv_channels)):\n",
    "            self.h1.add_module(\n",
    "                name=f'conv{i}',\n",
    "                module=nn.Conv2d(\n",
    "                    in_channels = 3 if i == 0 else conv_channels[i-1],\n",
    "                    out_channels=conv_channels[i],\n",
    "                    kernel_size=3,\n",
    "                    padding=1\n",
    "                )\n",
    "            )\n",
    "            self.h1.add_module(\n",
    "                name=f'activation{i}',\n",
    "                module=nn.ReLU()\n",
    "            )\n",
    "        self.h1.add_module(\n",
    "            name=f'maxpool',\n",
    "            module=nn.MaxPool2d(2)\n",
    "        )\n",
    "        #output of h1 before maxpool is 16 32x32 images. After maxpool, 16 16x16 images\n",
    "# h1_in: torch.Size([1024, 3, 32, 32])\n",
    "# res_block_in: torch.Size([1024, 16, 16, 16])\n",
    "# h2_in: torch.Size([1024, 64])\n",
    "        #Add Resblocks\n",
    "        res_block_in = conv_channels[0]//2\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[\n",
    "                ResBlock(\n",
    "                    in_chans=conv_channels[-1],\n",
    "                    out_chans=conv_channels[-1],\n",
    "                    nonlinearity=nonlinearity,\n",
    "                    dropout=dropout_p\n",
    "                ) for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "        #output of resblocks is 16 16x16 images\n",
    "        \n",
    "        # Add final layers\n",
    "        self.h2 = nn.Sequential()\n",
    "        self.h2.add_module(\n",
    "            name='final_batch_norm',\n",
    "            module=nn.BatchNorm2d(\n",
    "                num_features=conv_channels[-1]\n",
    "            )\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name='final_relu',\n",
    "            module=nn.ReLU()\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'dropout_head',\n",
    "            module=nn.Dropout(dropout_h)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'gap',\n",
    "            module=nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'flatten',\n",
    "            module=nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #output is 16 8x8 images\n",
    "        #   16 comes from conv_channels[-1]\n",
    "        #   8x8 comes from input_dim // 4\n",
    "        fc_in = conv_channels[-1] * (input_dim//4)**2\n",
    "        \n",
    "        for i in range(len(fc_channels)):\n",
    "            \n",
    "            self.h2.add_module(\n",
    "                name=f'fc{i}',\n",
    "                module=nn.Linear(\n",
    "                    in_features=fc_in if i == 0 else fc_channels[i-1],\n",
    "                    out_features=fc_channels[i]\n",
    "                )\n",
    "            )\n",
    "            if i < len(fc_channels) - 1:\n",
    "                self.h2.add_module(\n",
    "                    name = f'fc_activation{i}',\n",
    "                    module=nn.ReLU()\n",
    "                )\n",
    "        self.h2.add_module('softmax', nn.Softmax(dim=1))\n",
    "    def forward(self, x):\n",
    "        #print(f\"h1_in: {x.shape}\")\n",
    "        out = self.h1(x)\n",
    "        #print(f\"res_block_in: {out.shape}\")\n",
    "        out = self.resblocks(out)\n",
    "        #print(f\"h2_in: {out.shape}\")\n",
    "        out = self.h2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin init train_loader\n",
      "Batch Size: 24.0 MiB\n",
      "begin init fetcher\n",
      "Init time: 3.84 seconds\n",
      "Training ResNet\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         0          |      7.801986      |      2.273633      |      2.301289      |      1.216393      |      0.100500      |      0.000000      |      4.617005      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         1          |      5.993375      |      2.206537      |      2.272568      |      2.992483      |      0.157900      |     57.114429      |      5.320131      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         2          |      5.934211      |      2.155597      |      2.155074      |     -0.024278      |      0.296800      |     87.967072      |      6.021304      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         3          |      5.985555      |      2.096445      |      2.098531      |      0.099498      |      0.376900      |     26.987873      |      6.722476      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         4          |      5.945818      |      2.045327      |      2.028331      |     -0.830940      |      0.446100      |     18.360312      |      7.423647      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         5          |      5.905675      |      2.004147      |      1.999227      |     -0.245491      |      0.474000      |      6.254199      |      8.124819      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         6          |      5.880868      |      1.975203      |      2.009553      |      1.739094      |      0.459200      |     -3.122359      |      8.825991      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         7          |      5.921003      |      1.955364      |      1.952699      |     -0.136288      |      0.515800      |      8.818570      |      9.527163      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         8          |      5.919264      |      1.934438      |      1.938828      |      0.226921      |      0.527900      |      2.345867      |     10.228335      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         9          |      5.971511      |      1.920930      |      1.941632      |      1.077745      |      0.527300      |     -0.113654      |     10.929507      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         10         |      5.921787      |      1.906393      |      1.910570      |      0.219122      |      0.557400      |      5.588181      |     11.630679      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         11         |      5.936925      |      1.896345      |      1.900993      |      0.245095      |      0.570900      |      2.421955      |     12.331851      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         12         |      5.888906      |      1.877341      |      1.877847      |      0.026987      |      0.598400      |      4.816962      |     13.033022      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         13         |      5.992515      |      1.867194      |      1.857296      |     -0.530123      |      0.616800      |      3.074869      |     13.734194      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         14         |      5.895777      |      1.852033      |      1.880899      |      1.558595      |      0.587800      |     -4.701693      |     14.435366      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         15         |      5.949069      |      1.839635      |      1.828716      |     -0.593527      |      0.646500      |      4.815172      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         16         |      5.968417      |      1.828360      |      1.843161      |      0.809544      |      0.630100      |     -2.536733      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         17         |      5.910331      |      1.823583      |      1.840948      |      0.952258      |      0.628700      |     -2.753292      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         18         |      5.933036      |      1.811307      |      1.834742      |      1.293810      |      0.642000      |     -0.696061      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         19         |      5.947035      |      1.811395      |      1.844881      |      1.848678      |      0.625600      |     -3.232794      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         20         |      5.961164      |      1.801574      |      1.800020      |     -0.086272      |      0.671500      |      3.866972      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         21         |      5.882267      |      1.793587      |      1.802983      |      0.523877      |      0.670200      |     -0.193593      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         22         |      5.886744      |      1.787683      |      1.781577      |     -0.341581      |      0.690900      |      2.889055      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         23         |      5.966791      |      1.783250      |      1.804870      |      1.212395      |      0.664300      |     -3.850051      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         24         |      5.996225      |      1.779663      |      1.792002      |      0.693340      |      0.681100      |     -1.418434      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         25         |      5.874416      |      1.772987      |      1.786004      |      0.734208      |      0.686300      |     -0.665797      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         26         |      5.973992      |      1.768897      |      1.781039      |      0.686413      |      0.689000      |     -0.274998      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         27         |      5.957873      |      1.762753      |      1.764264      |      0.085717      |      0.710100      |      2.778988      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         28         |      5.948750      |      1.761539      |      1.772208      |      0.605616      |      0.700700      |     -1.323759      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         29         |      5.928810      |      1.756910      |      1.777199      |      1.154809      |      0.692800      |     -2.436278      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         30         |      5.927157      |      1.757125      |      1.802386      |      2.575864      |      0.667900      |     -5.942830      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         31         |      5.897241      |      1.750931      |      1.762663      |      0.670097      |      0.705900      |     -0.591472      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         32         |      5.990415      |      1.746177      |      1.762136      |      0.913982      |      0.710700      |      0.084492      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         33         |      5.919352      |      1.741401      |      1.790256      |      2.805496      |      0.683500      |     -3.827210      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         34         |      5.974981      |      1.739016      |      1.766094      |      1.557082      |      0.703600      |     -0.999013      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         35         |      5.941816      |      1.736503      |      1.758773      |      1.282460      |      0.716500      |      0.816098      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         36         |      5.924908      |      1.736205      |      1.774309      |      2.194696      |      0.697600      |     -2.637820      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         37         |      5.917880      |      1.733037      |      1.741654      |      0.497208      |      0.731400      |      2.079549      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         38         |      6.002014      |      1.730733      |      1.740769      |      0.579890      |      0.727900      |     -0.478532      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         39         |      5.944173      |      1.729210      |      1.751449      |      1.286063      |      0.718600      |     -1.750065      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         40         |      6.038871      |      1.726914      |      1.751747      |      1.437997      |      0.718000      |     -1.832097      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         41         |      5.990957      |      1.725542      |      1.748645      |      1.338835      |      0.722800      |     -1.175827      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         42         |      5.967497      |      1.719578      |      1.754209      |      2.013932      |      0.719200      |     -1.668034      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         43         |      5.933310      |      1.718833      |      1.737047      |      1.059672      |      0.736700      |      0.724644      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         44         |      6.002730      |      1.722135      |      1.725340      |      0.186117      |      0.745900      |      1.248809      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         45         |      5.969673      |      1.714285      |      1.752945      |      2.255142      |      0.717200      |     -3.847700      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         46         |      5.987303      |      1.714679      |      1.751337      |      2.137874      |      0.718800      |     -3.633190      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         47         |      5.912031      |      1.711961      |      1.724840      |      0.752324      |      0.744700      |     -0.160882      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         48         |      5.995829      |      1.710160      |      1.712658      |      0.146070      |      0.759500      |      1.823300      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         49         |      5.909975      |      1.708216      |      1.760071      |      3.035600      |      0.710600      |     -6.438447      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         50         |      5.913543      |      1.706937      |      1.723075      |      0.945441      |      0.746500      |     -1.711654      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         51         |      5.910887      |      1.710039      |      1.738328      |      1.654252      |      0.730800      |     -3.778801      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         52         |      5.964361      |      1.706874      |      1.731616      |      1.449551      |      0.738300      |     -2.791310      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         53         |      5.958638      |      1.705920      |      1.728441      |      1.320148      |      0.741200      |     -2.409480      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         54         |      6.074829      |      1.701895      |      1.723435      |      1.265643      |      0.746600      |     -1.698485      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         55         |      5.934785      |      1.698644      |      1.739369      |      2.397486      |      0.729300      |     -3.976301      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         56         |      5.959862      |      1.699051      |      1.713906      |      0.874292      |      0.758400      |     -0.144833      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         57         |      5.978238      |      1.698151      |      1.726583      |      1.674278      |      0.741700      |     -2.343644      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         58         |      5.989710      |      1.697956      |      1.725809      |      1.640385      |      0.744200      |     -2.014480      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         59         |      5.973988      |      1.694963      |      1.750341      |      3.267208      |      0.717300      |     -5.556283      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         60         |      5.989480      |      1.695231      |      1.746250      |      3.009572      |      0.719200      |     -5.306124      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         61         |      5.937907      |      1.690817      |      1.720533      |      1.757509      |      0.749100      |     -1.369322      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         62         |      5.977637      |      1.691319      |      1.744370      |      3.136679      |      0.721900      |     -4.950623      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         63         |      5.974650      |      1.690675      |      1.707294      |      0.983007      |      0.761300      |      0.236998      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         64         |      6.015495      |      1.691271      |      1.783850      |      5.473947      |      0.681600      |     -10.468934     |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         65         |      5.970373      |      1.689390      |      1.764852      |      4.466818      |      0.699700      |     -8.091419      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         66         |      5.950355      |      1.687249      |      1.699225      |      0.709823      |      0.773200      |      1.563117      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         67         |      5.950218      |      1.685124      |      1.746788      |      3.659308      |      0.721400      |     -6.699433      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         68         |      5.994828      |      1.685809      |      1.732959      |      2.796892      |      0.735000      |     -4.940510      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         69         |      6.035336      |      1.684619      |      1.717654      |      1.961001      |      0.749200      |     -3.103982      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         70         |      5.973435      |      1.684349      |      1.761757      |      4.595733      |      0.702600      |     -9.130881      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         71         |      5.980864      |      1.683544      |      1.704824      |      1.263989      |      0.766100      |     -0.918260      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         72         |      5.996981      |      1.682238      |      1.728498      |      2.749923      |      0.740100      |     -4.280912      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         73         |      6.073171      |      1.682020      |      1.692898      |      0.646678      |      0.776400      |      0.413864      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         74         |      6.018007      |      1.679801      |      1.737187      |      3.416277      |      0.729600      |     -6.027823      |     15.136538      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "    del resnet\n",
    "    del train_loader_cuda\n",
    "    del test_loader_cuda\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reset CUDA context\n",
    "start = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "batch_size = int(2**11)\n",
    "workers = 12\n",
    "cpu_prefetch = 39\n",
    "gpu_prefetch = 28\n",
    "\n",
    "print('begin init train_loader')\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=cpu_prefetch,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "X_batch = next(iter(train_loader))[0]\n",
    "dtype_size = X_batch.element_size()\n",
    "print(f\"Batch Size: {X_batch.element_size() * X_batch.nelement() / 1024**2} MiB\")\n",
    "\n",
    "\n",
    "print('begin init fetcher')\n",
    "train_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = train_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=gpu_prefetch\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=1)\n",
    "test_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = test_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=1\n",
    ")\n",
    "\n",
    "resnet = ResNet(\n",
    "    input_dim = 32,\n",
    "    conv_channels=[16,16],\n",
    "    n_blocks = 10,\n",
    "    fc_channels=[16,10],\n",
    "    dropout_h = 0.6,\n",
    "    dropout_p = 0.4\n",
    ").to(device=device)\n",
    "print(f\"Init time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "resnet.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader_cuda,\n",
    "    train_len=len(train_loader),\n",
    "    test_loader=test_loader_cuda,\n",
    "    test_len=len(test_loader),\n",
    "    test_size=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 1e-3, 'weight_decay': 3e-3},\n",
    "    print_epoch=1\n",
    ")\n",
    "\n",
    "\n",
    "resnet.plot_training(\"ResNet Training Curves\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
