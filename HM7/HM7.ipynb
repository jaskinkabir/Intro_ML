{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import gc\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchtnt.utils.data import CudaDataPrefetcher\n",
    "\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_758205/2313373535.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean = torch.load('data/mean.pt')\n",
      "/tmp/ipykernel_758205/2313373535.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  std = torch.load('data/std.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor([0.4914, 0.4822, 0.4465])\n",
      "Std:  tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "dl = False\n",
    "data_path = './data'\n",
    "\n",
    "# Load CIFAR-10 dataset with the simple transform\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transforms.ToTensor())\n",
    "try: \n",
    "    mean = torch.load('data/mean.pt')\n",
    "    std = torch.load('data/std.pt')\n",
    "except FileNotFoundError:\n",
    "    print(\"Computing Mean and Std\")\n",
    "    train_imgs = torch.stack([img for img, _ in cifar10_train], dim=3)#.to(device=device)\n",
    "    view = train_imgs.view(3, -1)#.to(device=device)\n",
    "\n",
    "    mean = train_imgs.view(3, -1).mean(dim=1)\n",
    "    std = train_imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    torch.save(mean, 'data/mean.pt')\n",
    "    torch.save(std, 'data/std.pt')\n",
    "\n",
    "# Define the transform with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    @classmethod\n",
    "    def compare_results(cls, results1, results2):\n",
    "        print('Comparing results:')\n",
    "        comparisons = {\n",
    "            'accuracy': 100*(results1['accuracy'] - results2['accuracy'])/results1['accuracy'],\n",
    "            'precision': 100*(results1['precision'] - results2['precision'])/results1['precision'],\n",
    "            'recall': 100*(results1['recall'] - results2['recall'])/results1['recall'],\n",
    "            'f1': 100*(results1['f1'] - results2['f1'])/results1['f1']\n",
    "        }\n",
    "        for key, value in comparisons.items():\n",
    "            print(f'{key}: {value} %')\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            return self.forward(x).argmax(dim=1)\n",
    "    \n",
    "    def train_model(\n",
    "        self,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        train_len,\n",
    "        test_len,\n",
    "        test_size,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.SGD,\n",
    "        optimizer_args = [],\n",
    "        optimizer_kwargs = {},\n",
    "        print_epoch=10,\n",
    "        header_epoch = 15\n",
    "    ):  \n",
    "        scaler = GradScaler(\"cuda\")\n",
    "        optimizer = optimizer(self.parameters(), *optimizer_args, **optimizer_kwargs)\n",
    "        training_time = 0\n",
    "        train_hist = torch.zeros(epochs, device=device)\n",
    "        test_hist = torch.zeros(epochs, device=device)\n",
    "        accuracy_hist = torch.zeros(epochs, device=device)\n",
    "        \n",
    "        cell_width = 20\n",
    "        header_form_spec = f'^{cell_width}'\n",
    "        \n",
    "        epoch_inspection = {\n",
    "            \"Epoch\": 0,\n",
    "            \"Epoch Time (s)\": 0,\n",
    "            \"Training Loss\": 0,\n",
    "            \"Test Loss \": 0,\n",
    "            \"Overfit (%)\": 0,\n",
    "            \"Accuracy (%)\": 0,\n",
    "            \"Δ Accuracy (%)\": 0,\n",
    "            \"GPU Memory (GiB)\": 0\n",
    "        }\n",
    "\n",
    "        header_string = \"|\"\n",
    "        for key in epoch_inspection.keys():\n",
    "            header_string += (f\"{key:{header_form_spec}}|\")\n",
    "        \n",
    "        divider_string = '-'*len(header_string)\n",
    "        if print_epoch:\n",
    "            print(f'Training {self.__class__.__name__}\\n')\n",
    "            print(divider_string)\n",
    "        max_accuracy = torch.zeros(1, device=device)            \n",
    "        for epoch in range(epochs):\n",
    "            begin_epoch = time.time()\n",
    "            self.train()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            train_loss = 0\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                #X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(\"cuda\"):\n",
    "                    Y_pred = self.forward(X_batch)\n",
    "                    loss = loss_fn(Y_pred, Y_batch)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss\n",
    "            training_time += time.time() - start_time\n",
    "            \n",
    "            train_loss = train_loss/train_len\n",
    "            train_hist[epoch] = train_loss\n",
    "            \n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = torch.zeros(1, device=device)\n",
    "                correct = torch.zeros(1, device=device)               \n",
    "                \n",
    "                for X_test_batch, Y_test_batch in test_loader:\n",
    "                    #X_test_batch, Y_test_batch = X_test_batch.to(device, non_blocking=True), Y_test_batch.to(device, non_blocking=True)\n",
    "                    \n",
    "                    out = self.forward(X_test_batch)\n",
    "                    test_loss += loss_fn(out, Y_test_batch)\n",
    "                    correct += (out.argmax(dim=1) == Y_test_batch).sum()\n",
    "                    \n",
    "            test_loss = test_loss/test_len\n",
    "            test_hist[epoch] = test_loss\n",
    "            accuracy = correct/test_size\n",
    "            accuracy_hist[epoch] = accuracy\n",
    "            end_epoch = time.time()\n",
    "            if print_epoch and (epoch % print_epoch == 0 or epoch == epochs - 1) :\n",
    "                mem = (torch.cuda.memory_allocated() + torch.cuda.memory_reserved())/1024**3\n",
    "                if header_epoch and epoch % header_epoch == 0:\n",
    "                    print(header_string)\n",
    "                    print(divider_string)\n",
    "                epoch_duration = end_epoch - begin_epoch\n",
    "                overfit = 100 * (test_loss - train_loss) / train_loss\n",
    "                d_accuracy = torch.zeros(1) if max_accuracy == 0 else 100 * (accuracy - max_accuracy) / max_accuracy\n",
    "                if accuracy > max_accuracy:\n",
    "                    max_accuracy = accuracy\n",
    "                \n",
    "                epoch_inspection['Epoch'] = f'{epoch}'\n",
    "                epoch_inspection['Epoch Time (s)'] = f'{epoch_duration:4f}'\n",
    "                epoch_inspection['Training Loss'] = f'{train_loss.item():8f}'\n",
    "                epoch_inspection['Test Loss '] = f'{test_loss.item():8f}'\n",
    "                epoch_inspection['Overfit (%)'] = f'{overfit.item():4f}'\n",
    "                epoch_inspection['Accuracy (%)'] = f'{accuracy.item():4f}'\n",
    "                epoch_inspection['Δ Accuracy (%)'] = f'{d_accuracy.item():4f}'\n",
    "                epoch_inspection[\"GPU Memory (GiB)\"] = f'{mem:2f}'\n",
    "                for value in epoch_inspection.values():\n",
    "                    print(f\"|{value:^{cell_width}}\", end='')\n",
    "                print('|')\n",
    "                print(divider_string)\n",
    "            \n",
    "\n",
    "        print(f'\\nTraining Time: {training_time} seconds\\n')\n",
    "        \n",
    "        self.train_hist = train_hist\n",
    "        self.test_hist = test_hist\n",
    "        self.accuracy_hist = accuracy_hist\n",
    "    \n",
    "    def plot_training(self, title='Training Results'):\n",
    "        plt.plot(self.train_hist, label='Training Loss')\n",
    "        plt.plot(self.test_hist, label='Test Loss')\n",
    "        plt.plot(self.accuracy_hist, label='Accuracy')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_results(self, Y_test=None, Y_pred=None):\n",
    "        if Y_test is None:\n",
    "            Y_test = self.last_test\n",
    "        if Y_pred is None:\n",
    "            Y_pred = self.last_pred\n",
    "            \n",
    "        if isinstance(Y_test, torch.Tensor):\n",
    "            Y_test = Y_test.cpu().detach().numpy()\n",
    "        if isinstance(Y_pred, torch.Tensor):\n",
    "            Y_pred = Y_pred.cpu().detach().numpy()\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(Y_test, Y_pred),\n",
    "            'precision': precision_score(Y_test, Y_pred, average='weighted'),\n",
    "            'recall': recall_score(Y_test, Y_pred, average='weighted'),\n",
    "            'f1': f1_score(Y_test, Y_pred, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(Y_test, Y_pred),\n",
    "            'classification_report': classification_report(Y_test, Y_pred)\n",
    "        }\n",
    "        self.last_results = results\n",
    "        return results\n",
    "    def print_results(self, results=None):\n",
    "        if results is None:\n",
    "            try: \n",
    "                results = self.last_results\n",
    "            except:\n",
    "                results = self.get_results()\n",
    "        for key, value in results.items():\n",
    "            if key in ['confusion_matrix', 'classification_report']:\n",
    "                print(f'{key.capitalize()}:\\n{value}')\n",
    "            else:\n",
    "                print(f'{key.capitalize()}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvImageClassifier(Classifier):\n",
    "    def __init__(self, input_dim, conv_layers, fc_layers, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stack = nn.Sequential(OrderedDict(\n",
    "            [\n",
    "                ('conv0', nn.Conv2d(in_channels=3, out_channels=conv_layers[0], kernel_size=3, padding=1)),\n",
    "                ('activation0', activation()),\n",
    "                ('maxpool0', nn.MaxPool2d(2)),\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "        for i in range(1, len(conv_layers)):\n",
    "            self.stack.add_module(f'conv{i}', nn.Conv2d(in_channels=conv_layers[i-1], out_channels=conv_layers[i], kernel_size=3, padding=1))\n",
    "            self.stack.add_module(f'activation{i}', activation())\n",
    "            self.stack.add_module(f'maxpool{i}', nn.MaxPool2d(2))\n",
    "            \n",
    "        conv_out = input_dim//(2**len(conv_layers))\n",
    "        self.stack.add_module('flatten', nn.Flatten())\n",
    "        self.stack.add_module(f'fc0', nn.Linear(conv_out**2*conv_layers[-1], fc_layers[0]))\n",
    "        \n",
    "        for i in range(1, len(fc_layers)):\n",
    "            self.stack.add_module(f'activation_fc{i}', nn.Tanh())\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(fc_layers[i-1], fc_layers[i]))        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "workers = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1a = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1a.stack)\n",
    "\n",
    "model_1a.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    train_len=len(cifar10_train),\n",
    "    test_loader=test_loader,\n",
    "    test_len=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 8e-3, 'weight_decay': 1e-2},\n",
    "    print_epoch=1,\n",
    "    header_epoch=15\n",
    ")\n",
    "\n",
    "del train_loader\n",
    "del test_loader\n",
    "del cifar10_train\n",
    "del cifar10_test\n",
    "\n",
    "model_1a.plot_training(\"2 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "workers = 16\n",
    "\n",
    "# Deal with loaders sticking around after interrupting training\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1b = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64, 128],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1b.stack)\n",
    "\n",
    "model_1b.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 3e-4, 'weight_decay': 1e-2}, #Increase alpha to 2 next time\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    print_epoch=1\n",
    ")\n",
    "del train_loader\n",
    "del test_loader\n",
    "model_1b.plot_training(\"3 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after last bn but before last weight\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, nonlinearity = 'relu', stride=1, dropout = 0.4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.conv2 = nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.shortcut = nn.Conv2d(in_chans, out_chans, kernel_size=1, stride=1, bias=False) if in_chans != out_chans else nn.Identity()\n",
    "        \n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "    def forward(self, x):\n",
    "        out = self.batch_norm1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        return out\n",
    "class ResNet(Classifier):\n",
    "    def __init__(self, input_dim = 32, n_blocks = 10, conv_channels = [32,16], fc_channels = [32, 10], dropout_p=0.4, dropout_h=0.4, nonlinearity='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add initial convolutions\n",
    "        self.h1 = nn.Sequential()\n",
    "        for i in range(len(conv_channels)):\n",
    "            self.h1.add_module(\n",
    "                name=f'conv{i}',\n",
    "                module=nn.Conv2d(\n",
    "                    in_channels = 3 if i == 0 else conv_channels[i-1],\n",
    "                    out_channels=conv_channels[i],\n",
    "                    kernel_size=3,\n",
    "                    padding=1\n",
    "                )\n",
    "            )\n",
    "            self.h1.add_module(\n",
    "                name=f'activation{i}',\n",
    "                module=nn.ReLU()\n",
    "            )\n",
    "        self.h1.add_module(\n",
    "            name=f'maxpool',\n",
    "            module=nn.MaxPool2d(2)\n",
    "        )\n",
    "        #output of h1 before maxpool is 16 32x32 images. After maxpool, 16 16x16 images\n",
    "# h1_in: torch.Size([1024, 3, 32, 32])\n",
    "# res_block_in: torch.Size([1024, 16, 16, 16])\n",
    "# h2_in: torch.Size([1024, 64])\n",
    "        #Add Resblocks\n",
    "        res_block_in = conv_channels[0]//2\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[\n",
    "                ResBlock(\n",
    "                    in_chans=conv_channels[-1],\n",
    "                    out_chans=conv_channels[-1],\n",
    "                    nonlinearity=nonlinearity,\n",
    "                    dropout=dropout_p\n",
    "                ) for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "        #output of resblocks is 16 16x16 images\n",
    "        \n",
    "        # Add final layers\n",
    "        self.h2 = nn.Sequential()\n",
    "        self.h2.add_module(\n",
    "            name='final_batch_norm',\n",
    "            module=nn.BatchNorm2d(\n",
    "                num_features=conv_channels[-1]\n",
    "            )\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name='final_relu',\n",
    "            module=nn.ReLU()\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'dropout_head',\n",
    "            module=nn.Dropout(dropout_h)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'gap',\n",
    "            module=nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'flatten',\n",
    "            module=nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #output is 16 8x8 images\n",
    "        #   16 comes from conv_channels[-1]\n",
    "        #   8x8 comes from input_dim // 4\n",
    "        fc_in = conv_channels[-1] * (input_dim//4)**2\n",
    "        \n",
    "        for i in range(len(fc_channels)):\n",
    "            \n",
    "            self.h2.add_module(\n",
    "                name=f'fc{i}',\n",
    "                module=nn.Linear(\n",
    "                    in_features=fc_in if i == 0 else fc_channels[i-1],\n",
    "                    out_features=fc_channels[i]\n",
    "                )\n",
    "            )\n",
    "            if i < len(fc_channels) - 1:\n",
    "                self.h2.add_module(\n",
    "                    name = f'fc_activation{i}',\n",
    "                    module=nn.ReLU()\n",
    "                )\n",
    "        self.h2.add_module('softmax', nn.Softmax(dim=1))\n",
    "    def forward(self, x):\n",
    "        #print(f\"h1_in: {x.shape}\")\n",
    "        out = self.h1(x)\n",
    "        #print(f\"res_block_in: {out.shape}\")\n",
    "        out = self.resblocks(out)\n",
    "        #print(f\"h2_in: {out.shape}\")\n",
    "        out = self.h2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin init train_loader\n",
      "Batch Size: 24.0 MiB\n",
      "begin init fetcher\n",
      "Init time: 3.97 seconds\n",
      "Training ResNet\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         0          |      7.458183      |      2.236908      |      2.203253      |     -1.504561      |      0.288800      |      0.000000      |      3.749182      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         1          |      5.581026      |      2.097247      |      2.060937      |     -1.731315      |      0.400800      |     38.781158      |      4.450356      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         2          |      5.592488      |      2.033731      |      2.013364      |     -1.001444      |      0.445400      |     11.127748      |      5.151528      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         3          |      5.528575      |      1.983319      |      1.964568      |     -0.945407      |      0.498900      |     12.011674      |      5.852700      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         4          |      5.516588      |      1.954776      |      1.958931      |      0.212540      |      0.507400      |      1.703744      |      6.553872      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         5          |      5.526659      |      1.933828      |      1.912453      |     -1.105354      |      0.553300      |      9.046115      |      7.255044      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         6          |      5.485538      |      1.917692      |      1.934151      |      0.858283      |      0.529300      |     -4.337609      |      7.956215      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         7          |      5.503021      |      1.899991      |      1.929780      |      1.567816      |      0.532800      |     -3.705044      |      8.657387      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         8          |      5.541918      |      1.888656      |      1.923374      |      1.838260      |      0.538400      |     -2.692928      |      9.358559      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         9          |      5.498485      |      1.882494      |      1.894929      |      0.660545      |      0.567900      |      2.638720      |     10.059731      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         10         |      5.443061      |      1.869313      |      1.862907      |     -0.342722      |      0.599200      |      5.511535      |     10.760903      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         11         |      5.540235      |      1.863777      |      1.861037      |     -0.147015      |      0.602200      |      0.500661      |     11.462075      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         12         |      5.515052      |      1.852339      |      1.895121      |      2.309624      |      0.567400      |     -5.778810      |     12.163247      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         13         |      5.594471      |      1.844460      |      1.872840      |      1.538683      |      0.587700      |     -2.407832      |     12.864419      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         14         |      5.491437      |      1.844886      |      1.890674      |      2.481878      |      0.568400      |     -5.612754      |     13.565590      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         15         |      5.526084      |      1.834979      |      1.866076      |      1.694688      |      0.597500      |     -0.780472      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         16         |      5.473922      |      1.829680      |      1.836856      |      0.392215      |      0.627600      |      4.217865      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         17         |      5.522658      |      1.824097      |      1.829517      |      0.297092      |      0.634800      |      1.147228      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         18         |      5.500313      |      1.822721      |      1.833217      |      0.575830      |      0.630900      |     -0.614365      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         19         |      5.514283      |      1.818854      |      1.828959      |      0.555564      |      0.634700      |     -0.015746      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         20         |      5.606882      |      1.811739      |      1.835073      |      1.287968      |      0.629800      |     -0.787649      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         21         |      5.569472      |      1.808132      |      1.808950      |      0.045241      |      0.655300      |      3.229364      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         22         |      5.534610      |      1.805699      |      1.814685      |      0.497673      |      0.649500      |     -0.885083      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         23         |      5.596009      |      1.803084      |      1.806802      |      0.206203      |      0.653700      |     -0.244158      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         24         |      5.518348      |      1.802056      |      1.822725      |      1.146993      |      0.641300      |     -2.136426      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         25         |      5.521101      |      1.797481      |      1.819351      |      1.216677      |      0.644400      |     -1.663354      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         26         |      5.539992      |      1.793465      |      1.789901      |     -0.198721      |      0.674400      |      2.914697      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         27         |      5.524078      |      1.790569      |      1.837252      |      2.607165      |      0.630000      |     -6.583627      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         28         |      5.512004      |      1.787193      |      1.831502      |      2.479272      |      0.630200      |     -6.553974      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         29         |      5.523277      |      1.787569      |      1.805003      |      0.975318      |      0.657400      |     -2.520753      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         30         |      5.556948      |      1.783618      |      1.810523      |      1.508454      |      0.654300      |     -2.980427      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         31         |      5.545117      |      1.783310      |      1.803410      |      1.127138      |      0.659100      |     -2.268680      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         32         |      5.574280      |      1.779308      |      1.827233      |      2.693435      |      0.633700      |     -6.034988      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         33         |      5.523752      |      1.778651      |      1.789720      |      0.622342      |      0.675700      |      0.192769      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         34         |      5.557600      |      1.773209      |      1.806089      |      1.854256      |      0.657200      |     -2.737906      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         35         |      5.578480      |      1.773587      |      1.799508      |      1.461474      |      0.663900      |     -1.746345      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         36         |      5.550074      |      1.769390      |      1.878661      |      6.175662      |      0.582200      |     -13.837505     |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         37         |      5.512774      |      1.769961      |      1.787730      |      1.003953      |      0.675400      |     -0.044406      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         38         |      5.543123      |      1.769698      |      1.807494      |      2.135719      |      0.652800      |     -3.389085      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         39         |      5.549383      |      1.763552      |      1.835998      |      4.107931      |      0.626400      |     -7.296139      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         40         |      5.725635      |      1.770027      |      1.795800      |      1.456089      |      0.668300      |     -1.095166      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         41         |      5.570467      |      1.769831      |      1.807532      |      2.130197      |      0.654600      |     -3.122694      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         42         |      5.617012      |      1.767055      |      1.805272      |      2.162749      |      0.658600      |     -2.530714      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         43         |      5.520400      |      1.766554      |      1.786854      |      1.149111      |      0.677700      |      0.295986      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         44         |      5.542429      |      1.766194      |      1.825521      |      3.359011      |      0.635200      |     -6.271214      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         45         |      5.594938      |      1.764920      |      1.822052      |      3.237046      |      0.637600      |     -5.917069      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         46         |      5.586484      |      1.764540      |      1.774070      |      0.540115      |      0.691200      |      1.992028      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         47         |      5.575024      |      1.764718      |      1.803739      |      2.211203      |      0.659200      |     -4.629622      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         48         |      5.577197      |      1.762297      |      1.791154      |      1.637424      |      0.672400      |     -2.719902      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         49         |      5.569074      |      1.767114      |      1.785529      |      1.042093      |      0.679800      |     -1.649303      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         50         |      5.543522      |      1.759613      |      1.823515      |      3.631616      |      0.635500      |     -8.058451      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         51         |      5.587270      |      1.757797      |      1.783800      |      1.479329      |      0.679200      |     -1.736106      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         52         |      5.559527      |      1.756780      |      1.833721      |      4.379655      |      0.628300      |     -9.100109      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         53         |      5.543371      |      1.757195      |      1.791238      |      1.937353      |      0.671600      |     -2.835644      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         54         |      5.565233      |      1.756351      |      1.766234      |      0.562696      |      0.696300      |      0.737849      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         55         |      5.568680      |      1.752463      |      1.821925      |      3.963717      |      0.640000      |     -8.085593      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         56         |      5.595104      |      1.757351      |      1.822469      |      3.705509      |      0.638900      |     -8.243572      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         57         |      5.641135      |      1.757246      |      1.825702      |      3.895665      |      0.635100      |     -8.789310      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         58         |      5.576334      |      1.758069      |      1.781013      |      1.305087      |      0.682600      |     -1.967544      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         59         |      5.692199      |      1.751557      |      1.817115      |      3.742873      |      0.643700      |     -7.554210      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         60         |      5.590058      |      1.754954      |      1.802505      |      2.709491      |      0.658900      |     -5.371249      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         61         |      5.589902      |      1.754241      |      1.798237      |      2.507985      |      0.663700      |     -4.681888      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         62         |      5.708225      |      1.752056      |      1.813156      |      3.487366      |      0.648800      |     -6.821774      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         63         |      5.657218      |      1.753100      |      1.807293      |      3.091227      |      0.653600      |     -6.132414      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         64         |      5.643518      |      1.755650      |      1.777688      |      1.255268      |      0.682200      |     -2.024992      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         65         |      5.669180      |      1.754550      |      1.803732      |      2.803116      |      0.658500      |     -5.428697      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         66         |      5.619473      |      1.748804      |      1.737866      |     -0.625425      |      0.728900      |      4.681888      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         67         |      5.625288      |      1.753236      |      1.844956      |      5.231489      |      0.616800      |     -15.379332     |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         68         |      5.714283      |      1.753715      |      1.817349      |      3.628528      |      0.643000      |     -11.784874     |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         69         |      5.591083      |      1.750215      |      1.787220      |      2.114267      |      0.675200      |     -7.367262      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         70         |      5.585808      |      1.751787      |      1.745293      |     -0.370710      |      0.718900      |     -1.371929      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         71         |      5.581873      |      1.749302      |      1.791443      |      2.409030      |      0.667500      |     -8.423652      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         72         |      5.579161      |      1.747366      |      1.803263      |      3.198956      |      0.657900      |     -9.740703      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         73         |      5.571981      |      1.748940      |      1.780637      |      1.812350      |      0.681300      |     -6.530385      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         74         |      5.672455      |      1.750881      |      1.781242      |      1.734031      |      0.679500      |     -6.777332      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         75         |      5.588191      |      1.747707      |      1.774339      |      1.523781      |      0.688300      |     -5.570038      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         76         |      5.558259      |      1.744686      |      1.811377      |      3.822517      |      0.649600      |     -10.879406     |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         77         |      5.555994      |      1.746895      |      1.806136      |      3.391269      |      0.656400      |     -9.946494      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         78         |      5.660416      |      1.746768      |      1.777220      |      1.743342      |      0.685900      |     -5.899298      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         79         |      5.604197      |      1.748605      |      1.786531      |      2.168982      |      0.676600      |     -7.175193      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         80         |      5.592804      |      1.750372      |      1.810717      |      3.447536      |      0.650600      |     -10.742215     |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         81         |      5.565825      |      1.745316      |      1.834517      |      5.110886      |      0.626900      |     -13.993690     |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         82         |      5.594321      |      1.748056      |      1.782599      |      1.976131      |      0.679500      |     -6.777332      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         83         |      5.605942      |      1.747343      |      1.765672      |      1.048952      |      0.701900      |     -3.704205      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         84         |      5.629441      |      1.745175      |      1.790681      |      2.607511      |      0.668100      |     -8.341331      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         85         |      5.608214      |      1.746966      |      1.799111      |      2.984883      |      0.660500      |     -9.383999      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         86         |      5.583293      |      1.749362      |      1.813536      |      3.668412      |      0.647300      |     -11.194945     |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         87         |      5.579494      |      1.750393      |      1.810712      |      3.446043      |      0.649600      |     -10.879406     |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         88         |      5.604746      |      1.747740      |      1.760310      |      0.719223      |      0.706800      |     -3.031962      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         89         |      5.608987      |      1.745293      |      1.772998      |      1.587430      |      0.690500      |     -5.268212      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   Δ Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         90         |      5.589784      |      1.739901      |      1.762625      |      1.306019      |      0.701800      |     -3.717927      |     14.266762      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "    del resnet\n",
    "    del train_loader_cuda\n",
    "    del test_loader_cuda\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reset CUDA context\n",
    "start = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "batch_size = int(2**11)\n",
    "workers = 12\n",
    "cpu_prefetch = 39\n",
    "gpu_prefetch = 28\n",
    "\n",
    "print('begin init train_loader')\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=cpu_prefetch,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "X_batch = next(iter(train_loader))[0]\n",
    "dtype_size = X_batch.element_size()\n",
    "print(f\"Batch Size: {X_batch.element_size() * X_batch.nelement() / 1024**2} MiB\")\n",
    "\n",
    "\n",
    "print('begin init fetcher')\n",
    "train_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = train_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=gpu_prefetch\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=1)\n",
    "test_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = test_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=1\n",
    ")\n",
    "\n",
    "resnet = ResNet(\n",
    "    input_dim = 32,\n",
    "    conv_channels=[16,8],\n",
    "    n_blocks = 10,\n",
    "    fc_channels=[16,10],\n",
    "    dropout_h = 0.6,\n",
    "    dropout_p = 0.4\n",
    ").to(device=device)\n",
    "print(f\"Init time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "resnet.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader_cuda,\n",
    "    train_len=len(train_loader),\n",
    "    test_loader=test_loader_cuda,\n",
    "    test_len=len(test_loader),\n",
    "    test_size=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 5e-3, 'weight_decay': 3e-3},\n",
    "    print_epoch=1\n",
    ")\n",
    "\n",
    "\n",
    "resnet.plot_training(\"ResNet Training Curves\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
