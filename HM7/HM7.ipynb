{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import gc\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchtnt.utils.data import CudaDataPrefetcher\n",
    "\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_634085/2313373535.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean = torch.load('data/mean.pt')\n",
      "/tmp/ipykernel_634085/2313373535.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  std = torch.load('data/std.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor([0.4914, 0.4822, 0.4465])\n",
      "Std:  tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "dl = False\n",
    "data_path = './data'\n",
    "\n",
    "# Load CIFAR-10 dataset with the simple transform\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transforms.ToTensor())\n",
    "try: \n",
    "    mean = torch.load('data/mean.pt')\n",
    "    std = torch.load('data/std.pt')\n",
    "except FileNotFoundError:\n",
    "    print(\"Computing Mean and Std\")\n",
    "    train_imgs = torch.stack([img for img, _ in cifar10_train], dim=3)#.to(device=device)\n",
    "    view = train_imgs.view(3, -1)#.to(device=device)\n",
    "\n",
    "    mean = train_imgs.view(3, -1).mean(dim=1)\n",
    "    std = train_imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    torch.save(mean, 'data/mean.pt')\n",
    "    torch.save(std, 'data/std.pt')\n",
    "\n",
    "# Define the transform with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    @classmethod\n",
    "    def compare_results(cls, results1, results2):\n",
    "        print('Comparing results:')\n",
    "        comparisons = {\n",
    "            'accuracy': 100*(results1['accuracy'] - results2['accuracy'])/results1['accuracy'],\n",
    "            'precision': 100*(results1['precision'] - results2['precision'])/results1['precision'],\n",
    "            'recall': 100*(results1['recall'] - results2['recall'])/results1['recall'],\n",
    "            'f1': 100*(results1['f1'] - results2['f1'])/results1['f1']\n",
    "        }\n",
    "        for key, value in comparisons.items():\n",
    "            print(f'{key}: {value} %')\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            return self.forward(x).argmax(dim=1)\n",
    "    def train_model_no_cpu(\n",
    "        self,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        train_len,\n",
    "        test_len,\n",
    "        test_size,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.SGD,\n",
    "        optimizer_args = [],\n",
    "        optimizer_kwargs = {},\n",
    "        **kwargs        \n",
    "    ):\n",
    "        scaler = GradScaler(\"cuda\")\n",
    "        optimizer = optimizer(self.parameters(), *optimizer_args, **optimizer_kwargs)\n",
    "        min_test_loss = float('inf')\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "\n",
    "            train_loss = torch.zeros(1).to(device)\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                #X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast(\"cuda\"):\n",
    "                    Y_pred = self.forward(X_batch)\n",
    "                    loss = loss_fn(Y_pred, Y_batch)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                #train_loss += loss\n",
    "            \n",
    "            #train_loss = train_loss/train_len\n",
    "            \n",
    "            \n",
    "            # self.eval()\n",
    "            # with torch.no_grad():\n",
    "            #     test_loss = torch.zeros(1).to(device)\n",
    "            #     #correct = torch.zeros(1).to(device)               \n",
    "                \n",
    "            #     for X_test_batch, Y_test_batch in test_loader:\n",
    "            #         #X_test_batch, Y_test_batch = X_test_batch.to(device, non_blocking=True), Y_test_batch.to(device, non_blocking=True)\n",
    "                    \n",
    "            #         out = self.forward(X_test_batch)\n",
    "            #         test_loss += loss_fn(out, Y_test_batch)\n",
    "                    #correct += (out.argmax(dim=1) == Y_test_batch).sum()\n",
    "                #test_loss = test_loss/test_len\n",
    "                #correct = correct/test_size\n",
    "                # if test_loss > min_test_loss:\n",
    "                #     print(f'Epoch: {epoch}, Test Loss: {test_loss}')\n",
    "                # else:\n",
    "                #     min_test_loss = test_loss\n",
    "                    \n",
    "    \n",
    "    def train_model(\n",
    "        self,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        train_len,\n",
    "        test_len,\n",
    "        test_size,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.SGD,\n",
    "        optimizer_args = [],\n",
    "        optimizer_kwargs = {},\n",
    "        print_epoch=10,\n",
    "        header_epoch = 15\n",
    "    ):  \n",
    "        scaler = GradScaler(\"cuda\")\n",
    "        optimizer = optimizer(self.parameters(), *optimizer_args, **optimizer_kwargs)\n",
    "        training_time = 0\n",
    "        train_hist = np.zeros(epochs)\n",
    "        test_hist = np.zeros(epochs)\n",
    "        accuracy_hist = np.zeros(epochs)\n",
    "        \n",
    "        cell_width = 20\n",
    "        header_form_spec = f'^{cell_width}'\n",
    "        \n",
    "        epoch_inspection = {\n",
    "            \"Epoch\": 0,\n",
    "            \"Epoch Time (s)\": 0,\n",
    "            \"Training Loss\": 0,\n",
    "            \"Test Loss \": 0,\n",
    "            \"Overfit (%)\": 0,\n",
    "            \"Accuracy (%)\": 0,\n",
    "            \"Δ Accuracy (%)\": 0,\n",
    "            \"GPU Memory (GiB)\": 0\n",
    "        }\n",
    "\n",
    "        header_string = \"|\"\n",
    "        for key in epoch_inspection.keys():\n",
    "            header_string += (f\"{key:{header_form_spec}}|\")\n",
    "        \n",
    "        divider_string = '-'*len(header_string)\n",
    "        if print_epoch:\n",
    "            print(f'Training {self.__class__.__name__}\\n')\n",
    "            print(divider_string)\n",
    "        max_accuracy = 0            \n",
    "        for epoch in range(epochs):\n",
    "            begin_epoch = time.time()\n",
    "            self.train()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            train_loss = 0\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                #X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast(\"cuda\"):\n",
    "                    Y_pred = self.forward(X_batch)\n",
    "                    loss = loss_fn(Y_pred, Y_batch)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.detach()\n",
    "            training_time += time.time() - start_time\n",
    "            \n",
    "            train_loss = train_loss/train_len\n",
    "            train_hist[epoch] = train_loss\n",
    "            \n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = 0\n",
    "                correct = 0               \n",
    "                \n",
    "                for X_test_batch, Y_test_batch in test_loader:\n",
    "                    #X_test_batch, Y_test_batch = X_test_batch.to(device, non_blocking=True), Y_test_batch.to(device, non_blocking=True)\n",
    "                    \n",
    "                    out = self.forward(X_test_batch)\n",
    "                    test_loss += loss_fn(out, Y_test_batch).detach()\n",
    "                    correct += (out.argmax(dim=1) == Y_test_batch).sum()\n",
    "                    \n",
    "            test_loss = test_loss/test_len\n",
    "            test_hist[epoch] = test_loss\n",
    "            accuracy = correct/test_size\n",
    "            accuracy_hist[epoch] = accuracy\n",
    "            end_epoch = time.time()\n",
    "            if print_epoch and (epoch % print_epoch == 0 or epoch == epochs - 1) :\n",
    "                mem = (torch.cuda.memory_allocated() + torch.cuda.memory_reserved())/1024**3\n",
    "                if header_epoch and epoch % header_epoch == 0:\n",
    "                    print(header_string)\n",
    "                    print(divider_string)\n",
    "                epoch_duration = end_epoch - begin_epoch\n",
    "                overfit = 100 * (test_loss - train_loss) / train_loss\n",
    "                d_accuracy = 0 if max_accuracy == 0 else 100 * (accuracy - max_accuracy) / max_accuracy\n",
    "                if accuracy > max_accuracy:\n",
    "                    max_accuracy = accuracy\n",
    "                \n",
    "                epoch_inspection['Epoch'] = f'{epoch}'\n",
    "                epoch_inspection['Epoch Time (s)'] = f'{epoch_duration:4f}'\n",
    "                epoch_inspection['Training Loss'] = f'{train_loss:8f}'\n",
    "                epoch_inspection['Test Loss '] = f'{test_loss:8f}'\n",
    "                epoch_inspection['Overfit (%)'] = f'{overfit:4f}'\n",
    "                epoch_inspection['Accuracy (%)'] = f'{accuracy:4f}'\n",
    "                epoch_inspection['Δ Accuracy (%)'] = f'{d_accuracy:4f}'\n",
    "                epoch_inspection[\"GPU Memory (GiB)\"] = f'{mem:2f}'\n",
    "                for value in epoch_inspection.values():\n",
    "                    print(f\"|{value:^{cell_width}}\", end='')\n",
    "                print('|')\n",
    "                print(divider_string)\n",
    "            \n",
    "\n",
    "        print(f'\\nTraining Time: {training_time} seconds\\n')\n",
    "        \n",
    "        self.train_hist = train_hist\n",
    "        self.test_hist = test_hist\n",
    "        self.accuracy_hist = accuracy_hist\n",
    "    \n",
    "    def plot_training(self, title='Training Results'):\n",
    "        plt.plot(self.train_hist, label='Training Loss')\n",
    "        plt.plot(self.test_hist, label='Test Loss')\n",
    "        plt.plot(self.accuracy_hist, label='Accuracy')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_results(self, Y_test=None, Y_pred=None):\n",
    "        if Y_test is None:\n",
    "            Y_test = self.last_test\n",
    "        if Y_pred is None:\n",
    "            Y_pred = self.last_pred\n",
    "            \n",
    "        if isinstance(Y_test, torch.Tensor):\n",
    "            Y_test = Y_test.cpu().detach().numpy()\n",
    "        if isinstance(Y_pred, torch.Tensor):\n",
    "            Y_pred = Y_pred.cpu().detach().numpy()\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(Y_test, Y_pred),\n",
    "            'precision': precision_score(Y_test, Y_pred, average='weighted'),\n",
    "            'recall': recall_score(Y_test, Y_pred, average='weighted'),\n",
    "            'f1': f1_score(Y_test, Y_pred, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(Y_test, Y_pred),\n",
    "            'classification_report': classification_report(Y_test, Y_pred)\n",
    "        }\n",
    "        self.last_results = results\n",
    "        return results\n",
    "    def print_results(self, results=None):\n",
    "        if results is None:\n",
    "            try: \n",
    "                results = self.last_results\n",
    "            except:\n",
    "                results = self.get_results()\n",
    "        for key, value in results.items():\n",
    "            if key in ['confusion_matrix', 'classification_report']:\n",
    "                print(f'{key.capitalize()}:\\n{value}')\n",
    "            else:\n",
    "                print(f'{key.capitalize()}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvImageClassifier(Classifier):\n",
    "    def __init__(self, input_dim, conv_layers, fc_layers, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stack = nn.Sequential(OrderedDict(\n",
    "            [\n",
    "                ('conv0', nn.Conv2d(in_channels=3, out_channels=conv_layers[0], kernel_size=3, padding=1)),\n",
    "                ('activation0', activation()),\n",
    "                ('maxpool0', nn.MaxPool2d(2)),\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "        for i in range(1, len(conv_layers)):\n",
    "            self.stack.add_module(f'conv{i}', nn.Conv2d(in_channels=conv_layers[i-1], out_channels=conv_layers[i], kernel_size=3, padding=1))\n",
    "            self.stack.add_module(f'activation{i}', activation())\n",
    "            self.stack.add_module(f'maxpool{i}', nn.MaxPool2d(2))\n",
    "            \n",
    "        conv_out = input_dim//(2**len(conv_layers))\n",
    "        self.stack.add_module('flatten', nn.Flatten())\n",
    "        self.stack.add_module(f'fc0', nn.Linear(conv_out**2*conv_layers[-1], fc_layers[0]))\n",
    "        \n",
    "        for i in range(1, len(fc_layers)):\n",
    "            self.stack.add_module(f'activation_fc{i}', nn.Tanh())\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(fc_layers[i-1], fc_layers[i]))        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "workers = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1a = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1a.stack)\n",
    "\n",
    "model_1a.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    train_len=len(cifar10_train),\n",
    "    test_loader=test_loader,\n",
    "    test_len=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 8e-3, 'weight_decay': 1e-2},\n",
    "    print_epoch=1,\n",
    "    header_epoch=15\n",
    ")\n",
    "\n",
    "del train_loader\n",
    "del test_loader\n",
    "del cifar10_train\n",
    "del cifar10_test\n",
    "\n",
    "model_1a.plot_training(\"2 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "workers = 16\n",
    "\n",
    "# Deal with loaders sticking around after interrupting training\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1b = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64, 128],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1b.stack)\n",
    "\n",
    "model_1b.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 3e-4, 'weight_decay': 1e-2}, #Increase alpha to 2 next time\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    print_epoch=1\n",
    ")\n",
    "del train_loader\n",
    "del test_loader\n",
    "model_1b.plot_training(\"3 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after last bn but before last weight\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, nonlinearity = 'relu', stride=1, dropout = 0.4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.conv2 = nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.shortcut = nn.Conv2d(in_chans, out_chans, kernel_size=1, stride=1, bias=False) if in_chans != out_chans else nn.Identity()\n",
    "        \n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "    def forward(self, x):\n",
    "        out = self.batch_norm1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        return out\n",
    "class ResNet(Classifier):\n",
    "    def __init__(self, input_dim = 32, n_blocks = 10, conv_channels = [32,16], fc_channels = [32, 10], dropout_p=0.4, dropout_h=0.4, nonlinearity='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add initial convolutions\n",
    "        self.h1 = nn.Sequential()\n",
    "        for i in range(len(conv_channels)):\n",
    "            self.h1.add_module(\n",
    "                name=f'conv{i}',\n",
    "                module=nn.Conv2d(\n",
    "                    in_channels = 3 if i == 0 else conv_channels[i-1],\n",
    "                    out_channels=conv_channels[i],\n",
    "                    kernel_size=3,\n",
    "                    padding=1\n",
    "                )\n",
    "            )\n",
    "            self.h1.add_module(\n",
    "                name=f'activation{i}',\n",
    "                module=nn.ReLU()\n",
    "            )\n",
    "        self.h1.add_module(\n",
    "            name=f'maxpool',\n",
    "            module=nn.MaxPool2d(2)\n",
    "        )\n",
    "        #output of h1 before maxpool is 16 32x32 images. After maxpool, 16 16x16 images\n",
    "# h1_in: torch.Size([1024, 3, 32, 32])\n",
    "# res_block_in: torch.Size([1024, 16, 16, 16])\n",
    "# h2_in: torch.Size([1024, 64])\n",
    "        #Add Resblocks\n",
    "        res_block_in = conv_channels[0]//2\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[\n",
    "                ResBlock(\n",
    "                    in_chans=conv_channels[-1],\n",
    "                    out_chans=conv_channels[-1],\n",
    "                    nonlinearity=nonlinearity,\n",
    "                    dropout=dropout_p\n",
    "                ) for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "        #output of resblocks is 16 16x16 images\n",
    "        \n",
    "        # Add final layers\n",
    "        self.h2 = nn.Sequential()\n",
    "        self.h2.add_module(\n",
    "            name='final_batch_norm',\n",
    "            module=nn.BatchNorm2d(\n",
    "                num_features=conv_channels[-1]\n",
    "            )\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name='final_relu',\n",
    "            module=nn.ReLU()\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'dropout_head',\n",
    "            module=nn.Dropout(dropout_h)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'gap',\n",
    "            module=nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'flatten',\n",
    "            module=nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #output is 16 8x8 images\n",
    "        #   16 comes from conv_channels[-1]\n",
    "        #   8x8 comes from input_dim // 4\n",
    "        fc_in = conv_channels[-1] * (input_dim//4)**2\n",
    "        \n",
    "        for i in range(len(fc_channels)):\n",
    "            \n",
    "            self.h2.add_module(\n",
    "                name=f'fc{i}',\n",
    "                module=nn.Linear(\n",
    "                    in_features=fc_in if i == 0 else fc_channels[i-1],\n",
    "                    out_features=fc_channels[i]\n",
    "                )\n",
    "            )\n",
    "            if i < len(fc_channels) - 1:\n",
    "                self.h2.add_module(\n",
    "                    name = f'fc_activation{i}',\n",
    "                    module=nn.ReLU()\n",
    "                )\n",
    "        self.h2.add_module('softmax', nn.Softmax(dim=1))\n",
    "    def forward(self, x):\n",
    "        #print(f\"h1_in: {x.shape}\")\n",
    "        out = self.h1(x)\n",
    "        #print(f\"res_block_in: {out.shape}\")\n",
    "        out = self.resblocks(out)\n",
    "        #print(f\"h2_in: {out.shape}\")\n",
    "        out = self.h2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin init train_loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 12.0 MiB\n",
      "begin init fetcher\n",
      "Init time: 5.26 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m\n\u001b[1;32m     55\u001b[0m resnet \u001b[38;5;241m=\u001b[39m ResNet(\n\u001b[1;32m     56\u001b[0m     input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     57\u001b[0m     conv_channels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m8\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     dropout_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n\u001b[1;32m     62\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInit time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \u001b[43mresnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model_no_cpu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcifar10_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5e-3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     75\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m resnet\u001b[38;5;241m.\u001b[39mplot_training(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet Training Curves\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#[m1,n1]x[m2,n2] is only possible if n1 == m2\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mClassifier.train_model_no_cpu\u001b[0;34m(self, epochs, train_loader, test_loader, train_len, test_len, test_size, loss_fn, optimizer, optimizer_args, optimizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     43\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 44\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtnt/utils/data/data_prefetcher.py:94\u001b[0m, in \u001b[0;36mCudaDataPrefetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCudaDataPrefetcher[Batch]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtnt/utils/data/data_prefetcher.py:70\u001b[0m, in \u001b[0;36mCudaDataPrefetcher._reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events: List[torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mEvent] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_stream \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mStream()\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_iterable)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1191\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1189\u001b[0m _utils\u001b[38;5;241m.\u001b[39msignal_handling\u001b[38;5;241m.\u001b[39m_set_SIGCHLD_handler()\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_pids_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1228\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# prime the prefetch loop\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers):\n\u001b[0;32m-> 1228\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_put_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1471\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_put_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1471\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:691\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/sampler.py:347\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    346\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_in_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/sampler.py:197\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n):\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandperm(n, generator\u001b[38;5;241m=\u001b[39mgenerator)\u001b[38;5;241m.\u001b[39mtolist()[\n\u001b[1;32m    199\u001b[0m         : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m%\u001b[39m n\n\u001b[1;32m    200\u001b[0m     ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "    del resnet\n",
    "    del train_loader_cuda\n",
    "    del test_loader_cuda\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reset CUDA context\n",
    "start = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "batch_size = int(2**10)\n",
    "workers = 24\n",
    "cpu_prefetch = 24\n",
    "gpu_prefetch = int(workers*cpu_prefetch*0.5)\n",
    "print('begin init train_loader')\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=cpu_prefetch,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "X_batch = next(iter(train_loader))[0]\n",
    "dtype_size = X_batch.element_size()\n",
    "print(f\"Batch Size: {X_batch.element_size() * X_batch.nelement() / 1024**2} MiB\")\n",
    "\n",
    "\n",
    "print('begin init fetcher')\n",
    "train_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = train_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=gpu_prefetch\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=1)\n",
    "test_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = test_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=1\n",
    ")\n",
    "\n",
    "resnet = ResNet(\n",
    "    input_dim = 32,\n",
    "    conv_channels=[16,8],\n",
    "    n_blocks = 10,\n",
    "    fc_channels=[16,10],\n",
    "    dropout_h = 0.6,\n",
    "    dropout_p = 0.4\n",
    ").to(device=device)\n",
    "print(f\"Init time: {(time.time() - start):.2f} seconds\")\n",
    "resnet.train_model_no_cpu(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader_cuda,\n",
    "    train_len=len(train_loader),\n",
    "    test_loader=test_loader_cuda,\n",
    "    test_len=len(test_loader),\n",
    "    test_size=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 5e-3, 'weight_decay': 5e-3},\n",
    "    print_epoch=50\n",
    ")\n",
    "resnet.plot_training(\"ResNet Training Curves\")\n",
    "\n",
    "#[m1,n1]x[m2,n2] is only possible if n1 == m2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
