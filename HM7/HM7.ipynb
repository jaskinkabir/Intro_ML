{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import gc\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchtnt.utils.data import CudaDataPrefetcher\n",
    "\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor([0.4914, 0.4822, 0.4465])\n",
      "Std:  tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_253712/1411884817.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean = torch.load('data/mean.pt')\n",
      "/tmp/ipykernel_253712/1411884817.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  std = torch.load('data/std.pt')\n"
     ]
    }
   ],
   "source": [
    "dl = False\n",
    "data_path = './data'\n",
    "batch_size = 1000  # Adjust batch size as needed\n",
    "\n",
    "# Define a simple transform to convert images to tensors\n",
    "simple_transform = transforms.ToTensor()\n",
    "\n",
    "# Load CIFAR-10 dataset with the simple transform\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=dl, transform=simple_transform)\n",
    "try: \n",
    "    mean = torch.load('data/mean.pt')\n",
    "    std = torch.load('data/std.pt')\n",
    "except FileNotFoundError:\n",
    "    print(\"Computing Mean and Std\")\n",
    "    train_imgs = torch.stack([img for img, _ in cifar10], dim=3)#.to(device=device)\n",
    "    view = train_imgs.view(3, -1)#.to(device=device)\n",
    "\n",
    "    mean = train_imgs.view(3, -1).mean(dim=1)\n",
    "    std = train_imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    torch.save(mean, 'data/mean.pt')\n",
    "    torch.save(std, 'data/std.pt')\n",
    "\n",
    "# Define the transform with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    @classmethod\n",
    "    def compare_results(cls, results1, results2):\n",
    "        print('Comparing results:')\n",
    "        comparisons = {\n",
    "            'accuracy': 100*(results1['accuracy'] - results2['accuracy'])/results1['accuracy'],\n",
    "            'precision': 100*(results1['precision'] - results2['precision'])/results1['precision'],\n",
    "            'recall': 100*(results1['recall'] - results2['recall'])/results1['recall'],\n",
    "            'f1': 100*(results1['f1'] - results2['f1'])/results1['f1']\n",
    "        }\n",
    "        for key, value in comparisons.items():\n",
    "            print(f'{key}: {value} %')\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            return self.forward(x).argmax(dim=1)\n",
    "    \n",
    "    def train_model(\n",
    "        self,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        train_len,\n",
    "        test_len,\n",
    "        test_size,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.SGD,\n",
    "        optimizer_args = [],\n",
    "        optimizer_kwargs = {},\n",
    "        print_epoch=10,\n",
    "        header_epoch = 15\n",
    "    ):  \n",
    "        scaler = GradScaler(\"cuda\")\n",
    "        optimizer = optimizer(self.parameters(), *optimizer_args, **optimizer_kwargs)\n",
    "        training_time = 0\n",
    "        train_hist = np.zeros(epochs)\n",
    "        test_hist = np.zeros(epochs)\n",
    "        accuracy_hist = np.zeros(epochs)\n",
    "        \n",
    "        cell_width = 17\n",
    "        header_form_spec = f'^{cell_width}'\n",
    "        \n",
    "        epoch_inspection = {\n",
    "            \"Epoch\": 0,\n",
    "            \"Epoch Time (s)\": 0,\n",
    "            \"Training Loss\": 0,\n",
    "            \"Test Loss \": 0,\n",
    "            \"Overfit (%)\": 0,\n",
    "            \"Accuracy (%)\": 0,\n",
    "            \"D Accuracy (%)\": 0,\n",
    "            \"Memory (GiB)\": 0\n",
    "            \n",
    "        }\n",
    "\n",
    "        header_string = \"|\"\n",
    "        for key in epoch_inspection.keys():\n",
    "            header_string += (f\"{key:{header_form_spec}}|\")\n",
    "        \n",
    "        divider_string = '-'*len(header_string)\n",
    "        if print_epoch:\n",
    "            print(f'Training {self.__class__.__name__}\\n')\n",
    "            print(divider_string)\n",
    "        max_accuracy = 0            \n",
    "        for epoch in range(epochs):\n",
    "            begin_epoch = time.time()\n",
    "            self.train()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            train_loss = 0\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                #X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast(\"cuda\"):\n",
    "                    Y_pred = self.forward(X_batch)\n",
    "                    loss = loss_fn(Y_pred, Y_batch)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.detach()\n",
    "            training_time += time.time() - start_time\n",
    "            \n",
    "            train_loss = train_loss/train_len\n",
    "            train_hist[epoch] = train_loss\n",
    "            \n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = 0\n",
    "                correct = 0               \n",
    "                \n",
    "                for X_test_batch, Y_test_batch in test_loader:\n",
    "                    #X_test_batch, Y_test_batch = X_test_batch.to(device, non_blocking=True), Y_test_batch.to(device, non_blocking=True)\n",
    "                    \n",
    "                    out = self.forward(X_test_batch)\n",
    "                    test_loss += loss_fn(out, Y_test_batch).detach()\n",
    "                    correct += (out.argmax(dim=1) == Y_test_batch).sum()\n",
    "                    \n",
    "            test_loss = test_loss/test_len\n",
    "            test_hist[epoch] = test_loss\n",
    "            accuracy = correct/test_size\n",
    "            accuracy_hist[epoch] = accuracy\n",
    "            end_epoch = time.time()\n",
    "            if print_epoch and (epoch % print_epoch == 0 or epoch == epochs - 1) :\n",
    "                mem = (torch.cuda.memory_allocated() + torch.cuda.memory_reserved())/1024**3\n",
    "                if header_epoch and epoch % header_epoch == 0:\n",
    "                    print(header_string)\n",
    "                    print(divider_string)\n",
    "                epoch_duration = end_epoch - begin_epoch\n",
    "                overfit = 100 * (test_loss - train_loss) / train_loss\n",
    "                d_accuracy = 0 if max_accuracy == 0 else 100 * (accuracy - max_accuracy) / max_accuracy\n",
    "                if accuracy > max_accuracy:\n",
    "                    max_accuracy = accuracy\n",
    "                \n",
    "                epoch_inspection['Epoch'] = f'{epoch}'\n",
    "                epoch_inspection['Epoch Time (s)'] = f'{epoch_duration:4f}'\n",
    "                epoch_inspection['Training Loss'] = f'{train_loss:8f}'\n",
    "                epoch_inspection['Test Loss '] = f'{test_loss:8f}'\n",
    "                epoch_inspection['Overfit (%)'] = f'{overfit:4f}'\n",
    "                epoch_inspection['Accuracy (%)'] = f'{accuracy:4f}'\n",
    "                epoch_inspection['D Accuracy (%)'] = f'{d_accuracy:4f}'\n",
    "                epoch_inspection['Memory (GiB)'] = f'{mem:2f}'\n",
    "                for value in epoch_inspection.values():\n",
    "                    print(f\"|{value:^{cell_width}}\", end='')\n",
    "                print('|')\n",
    "                print(divider_string)\n",
    "            \n",
    "\n",
    "        print(f'\\nTraining Time: {training_time} seconds\\n')\n",
    "        \n",
    "        self.train_hist = train_hist\n",
    "        self.test_hist = test_hist\n",
    "        self.accuracy_hist = accuracy_hist\n",
    "    \n",
    "    def plot_training(self, title='Training Results'):\n",
    "        plt.plot(self.train_hist, label='Training Loss')\n",
    "        plt.plot(self.test_hist, label='Test Loss')\n",
    "        plt.plot(self.accuracy_hist, label='Accuracy')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_results(self, Y_test=None, Y_pred=None):\n",
    "        if Y_test is None:\n",
    "            Y_test = self.last_test\n",
    "        if Y_pred is None:\n",
    "            Y_pred = self.last_pred\n",
    "            \n",
    "        if isinstance(Y_test, torch.Tensor):\n",
    "            Y_test = Y_test.cpu().detach().numpy()\n",
    "        if isinstance(Y_pred, torch.Tensor):\n",
    "            Y_pred = Y_pred.cpu().detach().numpy()\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(Y_test, Y_pred),\n",
    "            'precision': precision_score(Y_test, Y_pred, average='weighted'),\n",
    "            'recall': recall_score(Y_test, Y_pred, average='weighted'),\n",
    "            'f1': f1_score(Y_test, Y_pred, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(Y_test, Y_pred),\n",
    "            'classification_report': classification_report(Y_test, Y_pred)\n",
    "        }\n",
    "        self.last_results = results\n",
    "        return results\n",
    "    def print_results(self, results=None):\n",
    "        if results is None:\n",
    "            try: \n",
    "                results = self.last_results\n",
    "            except:\n",
    "                results = self.get_results()\n",
    "        for key, value in results.items():\n",
    "            if key in ['confusion_matrix', 'classification_report']:\n",
    "                print(f'{key.capitalize()}:\\n{value}')\n",
    "            else:\n",
    "                print(f'{key.capitalize()}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvImageClassifier(Classifier):\n",
    "    def __init__(self, input_dim, conv_layers, fc_layers, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stack = nn.Sequential(OrderedDict(\n",
    "            [\n",
    "                ('conv0', nn.Conv2d(in_channels=3, out_channels=conv_layers[0], kernel_size=3, padding=1)),\n",
    "                ('activation0', activation()),\n",
    "                ('maxpool0', nn.MaxPool2d(2)),\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "        for i in range(1, len(conv_layers)):\n",
    "            self.stack.add_module(f'conv{i}', nn.Conv2d(in_channels=conv_layers[i-1], out_channels=conv_layers[i], kernel_size=3, padding=1))\n",
    "            self.stack.add_module(f'activation{i}', activation())\n",
    "            self.stack.add_module(f'maxpool{i}', nn.MaxPool2d(2))\n",
    "            \n",
    "        conv_out = input_dim//(2**len(conv_layers))\n",
    "        self.stack.add_module('flatten', nn.Flatten())\n",
    "        self.stack.add_module(f'fc0', nn.Linear(conv_out**2*conv_layers[-1], fc_layers[0]))\n",
    "        \n",
    "        for i in range(1, len(fc_layers)):\n",
    "            self.stack.add_module(f'activation_fc{i}', nn.Tanh())\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(fc_layers[i-1], fc_layers[i]))        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "workers = 16\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1a = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1a.stack)\n",
    "\n",
    "model_1a.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    train_len=len(cifar10_train),\n",
    "    test_loader=test_loader,\n",
    "    test_len=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 8e-3, 'weight_decay': 1e-2},\n",
    "    print_epoch=1,\n",
    "    header_epoch=15\n",
    ")\n",
    "\n",
    "del train_loader\n",
    "del test_loader\n",
    "del cifar10_train\n",
    "del cifar10_test\n",
    "\n",
    "model_1a.plot_training(\"2 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "workers = 16\n",
    "\n",
    "# Deal with loaders sticking around after interrupting training\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1b = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64, 128],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1b.stack)\n",
    "\n",
    "model_1b.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 3e-4, 'weight_decay': 1e-2}, #Increase alpha to 2 next time\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    print_epoch=1\n",
    ")\n",
    "del train_loader\n",
    "del test_loader\n",
    "model_1b.plot_training(\"3 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_chans = 32, out_chans = 32, nonlinearity = 'relu', stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        # self.conv2 = nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        # self.batch_norm2 = nn.BatchNorm2d(num_features=in_chans)\n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        # torch.nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=nonlinearity)\n",
    "        # torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        #out = F.relu(out)\n",
    "        # out = self.conv2(out)\n",
    "        # out = self.batch_norm2(out)\n",
    "        out = out + x\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "class ResNet(Classifier):\n",
    "    def __init__(self, input_dim = 32, n_blocks = 10, conv_channels = [32], fc_channels = [32, 10], nonlinearity='relu'):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential()\n",
    "        # Add initial convolutions\n",
    "        for i in range(len(conv_channels)):\n",
    "            self.stack.add_module(\n",
    "                name=f'conv{i}',\n",
    "                module=nn.Conv2d(\n",
    "                    in_channels=3 if i == 0 else conv_channels[i-1],\n",
    "                    out_channels=conv_channels[i],\n",
    "                    kernel_size=3,\n",
    "                    padding=1\n",
    "                )\n",
    "            )\n",
    "            self.stack.add_module(\n",
    "                name=f'activation{i}',\n",
    "                module=nn.ReLU()\n",
    "            )\n",
    "            self.stack.add_module(\n",
    "                name=f'maxpool{i}',\n",
    "                module=nn.MaxPool2d(2)\n",
    "            )\n",
    "        \n",
    "        #Add Resblocks\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(in_chans=conv_channels[-1], nonlinearity=nonlinearity)])\n",
    "        )\n",
    "        self.stack.add_module('resblocks', self.resblocks)\n",
    "        self.stack.add_module('maxpool_blocks', nn.MaxPool2d(2))\n",
    "        \n",
    "        # Add final layers\n",
    "        conv_out = input_dim//(2**(len(conv_channels)+1))\n",
    "        self.stack.add_module('flatten', nn.Flatten())\n",
    "        \n",
    "        for i in range(len(fc_channels)):\n",
    "            \n",
    "            self.stack.add_module(\n",
    "                name=f'fc{i}',\n",
    "                module=nn.Linear(\n",
    "                    in_features=conv_channels[-1] * conv_out**2 if i == 0 else fc_channels[i-1],\n",
    "                    out_features=fc_channels[i]\n",
    "                )\n",
    "            )\n",
    "            if i < len(fc_channels) - 1:\n",
    "                self.stack.add_module(\n",
    "                    name = f'fc_activation{i}',\n",
    "                    module=nn.ReLU()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin init train_loader\n",
      "begin init fetcher\n",
      "Init time: 1.95 seconds\n",
      "Training ResNet\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|      Epoch      | Epoch Time (s)  |  Training Loss  |   Test Loss     |   Overfit (%)   |  Accuracy (%)   | D Accuracy (%)  |  Memory (GiB)   |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        0        |    8.212212     |    2.651390     |    2.317832     |   -12.580482    |    0.114400     |    0.000000     |    8.232270     |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        1        |    5.766369     |    2.317954     |    2.295060     |    -0.987667    |    0.110600     |    -3.321682    |    8.933443     |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        2        |    5.762543     |    2.204303     |    2.288001     |    3.797018     |    0.119100     |    4.108389     |    9.634615     |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        3        |    5.775087     |    2.155022     |    2.297153     |    6.595354     |    0.120100     |    0.839632     |    10.335787    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        4        |    5.795252     |    2.097721     |    2.316998     |    10.453115    |    0.120300     |    0.166525     |    11.036959    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        5        |    5.762960     |    2.054178     |    2.342383     |    14.030169    |    0.127100     |    5.652532     |    11.738131    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        6        |    5.807130     |    2.003165     |    2.330934     |    16.362583    |    0.124600     |    -1.966953    |    12.439302    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        7        |    5.802742     |    1.944335     |    2.322776     |    19.463751    |    0.131600     |    3.540521     |    13.140474    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        8        |    5.778549     |    1.889927     |    2.350760     |    24.383648    |    0.139100     |    5.699094     |    13.841646    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        9        |    5.794761     |    1.845940     |    2.385567     |    29.233217    |    0.143400     |    3.091300     |    14.542818    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 63\u001b[0m\n\u001b[1;32m     56\u001b[0m resnet \u001b[38;5;241m=\u001b[39m ResNet(\n\u001b[1;32m     57\u001b[0m     input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     58\u001b[0m     conv_channels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m],\n\u001b[1;32m     59\u001b[0m     n_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     60\u001b[0m     fc_channels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m     61\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInit time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[43mresnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcifar10_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     74\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m resnet\u001b[38;5;241m.\u001b[39mplot_training(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet Training Curves\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m, in \u001b[0;36mClassifier.train_model\u001b[0;34m(self, epochs, train_loader, test_loader, train_len, test_len, test_size, loss_fn, optimizer, optimizer_args, optimizer_kwargs, print_epoch, header_epoch)\u001b[0m\n\u001b[1;32m     81\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(Y_pred, Y_batch)\n\u001b[1;32m     82\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 83\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     86\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "    del resnet\n",
    "    del train_loader_cuda\n",
    "    del test_loader_cuda\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reset CUDA context\n",
    "start = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "batch_size = int(2**13)\n",
    "workers = 11\n",
    "cpu_prefetch = 20\n",
    "gpu_prefetch = 10\n",
    "\n",
    "print('begin init train_loader')\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=cpu_prefetch,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# X_batch = next(iter(train_loader))[0]\n",
    "# dtype_size = X_batch.element_size()\n",
    "# print(f\"Batch Size: {X_batch.element_size() * X_batch.nelement() / 1024**2} MiB\")\n",
    "\n",
    "\n",
    "print('begin init fetcher')\n",
    "train_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = train_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=gpu_prefetch\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=1)\n",
    "test_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = test_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=1\n",
    ")\n",
    "\n",
    "resnet = ResNet(\n",
    "    input_dim = 32,\n",
    "    conv_channels=[32],\n",
    "    n_blocks = 10,\n",
    "    fc_channels=[32, 10],\n",
    ").to(device=device)\n",
    "print(f\"Init time: {(time.time() - start):.2f} seconds\")\n",
    "resnet.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader_cuda,\n",
    "    train_len=len(train_loader),\n",
    "    test_loader=test_loader_cuda,\n",
    "    test_len=len(test_loader),\n",
    "    test_size=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 1e-4, 'weight_decay': 1e-1},\n",
    "    print_epoch=1\n",
    ")\n",
    "resnet.plot_training(\"ResNet Training Curves\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
