{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import gc\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchtnt.utils.data import CudaDataPrefetcher\n",
    "\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_479169/2313373535.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean = torch.load('data/mean.pt')\n",
      "/tmp/ipykernel_479169/2313373535.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  std = torch.load('data/std.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor([0.4914, 0.4822, 0.4465])\n",
      "Std:  tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "dl = False\n",
    "data_path = './data'\n",
    "\n",
    "# Load CIFAR-10 dataset with the simple transform\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transforms.ToTensor())\n",
    "try: \n",
    "    mean = torch.load('data/mean.pt')\n",
    "    std = torch.load('data/std.pt')\n",
    "except FileNotFoundError:\n",
    "    print(\"Computing Mean and Std\")\n",
    "    train_imgs = torch.stack([img for img, _ in cifar10_train], dim=3)#.to(device=device)\n",
    "    view = train_imgs.view(3, -1)#.to(device=device)\n",
    "\n",
    "    mean = train_imgs.view(3, -1).mean(dim=1)\n",
    "    std = train_imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    torch.save(mean, 'data/mean.pt')\n",
    "    torch.save(std, 'data/std.pt')\n",
    "\n",
    "# Define the transform with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    @classmethod\n",
    "    def compare_results(cls, results1, results2):\n",
    "        print('Comparing results:')\n",
    "        comparisons = {\n",
    "            'accuracy': 100*(results1['accuracy'] - results2['accuracy'])/results1['accuracy'],\n",
    "            'precision': 100*(results1['precision'] - results2['precision'])/results1['precision'],\n",
    "            'recall': 100*(results1['recall'] - results2['recall'])/results1['recall'],\n",
    "            'f1': 100*(results1['f1'] - results2['f1'])/results1['f1']\n",
    "        }\n",
    "        for key, value in comparisons.items():\n",
    "            print(f'{key}: {value} %')\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            return self.forward(x).argmax(dim=1)\n",
    "    \n",
    "    def train_model(\n",
    "        self,\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        train_len,\n",
    "        test_len,\n",
    "        test_size,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.SGD,\n",
    "        optimizer_args = [],\n",
    "        optimizer_kwargs = {},\n",
    "        print_epoch=10,\n",
    "        header_epoch = 15\n",
    "    ):  \n",
    "        scaler = GradScaler(\"cuda\")\n",
    "        optimizer = optimizer(self.parameters(), *optimizer_args, **optimizer_kwargs)\n",
    "        training_time = 0\n",
    "        train_hist = np.zeros(epochs)\n",
    "        test_hist = np.zeros(epochs)\n",
    "        accuracy_hist = np.zeros(epochs)\n",
    "        \n",
    "        cell_width = 20\n",
    "        header_form_spec = f'^{cell_width}'\n",
    "        \n",
    "        epoch_inspection = {\n",
    "            \"Epoch\": 0,\n",
    "            \"Epoch Time (s)\": 0,\n",
    "            \"Training Loss\": 0,\n",
    "            \"Test Loss \": 0,\n",
    "            \"Overfit (%)\": 0,\n",
    "            \"Accuracy (%)\": 0,\n",
    "            \"Δ Accuracy (%)\": 0,\n",
    "            \"GPU Memory (GiB)\": 0\n",
    "        }\n",
    "\n",
    "        header_string = \"|\"\n",
    "        for key in epoch_inspection.keys():\n",
    "            header_string += (f\"{key:{header_form_spec}}|\")\n",
    "        \n",
    "        divider_string = '-'*len(header_string)\n",
    "        if print_epoch:\n",
    "            print(f'Training {self.__class__.__name__}\\n')\n",
    "            print(divider_string)\n",
    "        max_accuracy = 0            \n",
    "        for epoch in range(epochs):\n",
    "            begin_epoch = time.time()\n",
    "            self.train()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            train_loss = 0\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                #X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast(\"cuda\"):\n",
    "                    Y_pred = self.forward(X_batch)\n",
    "                    loss = loss_fn(Y_pred, Y_batch)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.detach()\n",
    "            training_time += time.time() - start_time\n",
    "            \n",
    "            train_loss = train_loss/train_len\n",
    "            train_hist[epoch] = train_loss\n",
    "            \n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = 0\n",
    "                correct = 0               \n",
    "                \n",
    "                for X_test_batch, Y_test_batch in test_loader:\n",
    "                    #X_test_batch, Y_test_batch = X_test_batch.to(device, non_blocking=True), Y_test_batch.to(device, non_blocking=True)\n",
    "                    \n",
    "                    out = self.forward(X_test_batch)\n",
    "                    test_loss += loss_fn(out, Y_test_batch).detach()\n",
    "                    correct += (out.argmax(dim=1) == Y_test_batch).sum()\n",
    "                    \n",
    "            test_loss = test_loss/test_len\n",
    "            test_hist[epoch] = test_loss\n",
    "            accuracy = correct/test_size\n",
    "            accuracy_hist[epoch] = accuracy\n",
    "            end_epoch = time.time()\n",
    "            if print_epoch and (epoch % print_epoch == 0 or epoch == epochs - 1) :\n",
    "                mem = (torch.cuda.memory_allocated() + torch.cuda.memory_reserved())/1024**3\n",
    "                if header_epoch and epoch % header_epoch == 0:\n",
    "                    print(header_string)\n",
    "                    print(divider_string)\n",
    "                epoch_duration = end_epoch - begin_epoch\n",
    "                overfit = 100 * (test_loss - train_loss) / train_loss\n",
    "                d_accuracy = 0 if max_accuracy == 0 else 100 * (accuracy - max_accuracy) / max_accuracy\n",
    "                if accuracy > max_accuracy:\n",
    "                    max_accuracy = accuracy\n",
    "                \n",
    "                epoch_inspection['Epoch'] = f'{epoch}'\n",
    "                epoch_inspection['Epoch Time (s)'] = f'{epoch_duration:4f}'\n",
    "                epoch_inspection['Training Loss'] = f'{train_loss:8f}'\n",
    "                epoch_inspection['Test Loss '] = f'{test_loss:8f}'\n",
    "                epoch_inspection['Overfit (%)'] = f'{overfit:4f}'\n",
    "                epoch_inspection['Accuracy (%)'] = f'{accuracy:4f}'\n",
    "                epoch_inspection['Δ Accuracy (%)'] = f'{d_accuracy:4f}'\n",
    "                epoch_inspection[\"GPU Memory (GiB)\"] = f'{mem:2f}'\n",
    "                for value in epoch_inspection.values():\n",
    "                    print(f\"|{value:^{cell_width}}\", end='')\n",
    "                print('|')\n",
    "                print(divider_string)\n",
    "            \n",
    "\n",
    "        print(f'\\nTraining Time: {training_time} seconds\\n')\n",
    "        \n",
    "        self.train_hist = train_hist\n",
    "        self.test_hist = test_hist\n",
    "        self.accuracy_hist = accuracy_hist\n",
    "    \n",
    "    def plot_training(self, title='Training Results'):\n",
    "        plt.plot(self.train_hist, label='Training Loss')\n",
    "        plt.plot(self.test_hist, label='Test Loss')\n",
    "        plt.plot(self.accuracy_hist, label='Accuracy')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_results(self, Y_test=None, Y_pred=None):\n",
    "        if Y_test is None:\n",
    "            Y_test = self.last_test\n",
    "        if Y_pred is None:\n",
    "            Y_pred = self.last_pred\n",
    "            \n",
    "        if isinstance(Y_test, torch.Tensor):\n",
    "            Y_test = Y_test.cpu().detach().numpy()\n",
    "        if isinstance(Y_pred, torch.Tensor):\n",
    "            Y_pred = Y_pred.cpu().detach().numpy()\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(Y_test, Y_pred),\n",
    "            'precision': precision_score(Y_test, Y_pred, average='weighted'),\n",
    "            'recall': recall_score(Y_test, Y_pred, average='weighted'),\n",
    "            'f1': f1_score(Y_test, Y_pred, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(Y_test, Y_pred),\n",
    "            'classification_report': classification_report(Y_test, Y_pred)\n",
    "        }\n",
    "        self.last_results = results\n",
    "        return results\n",
    "    def print_results(self, results=None):\n",
    "        if results is None:\n",
    "            try: \n",
    "                results = self.last_results\n",
    "            except:\n",
    "                results = self.get_results()\n",
    "        for key, value in results.items():\n",
    "            if key in ['confusion_matrix', 'classification_report']:\n",
    "                print(f'{key.capitalize()}:\\n{value}')\n",
    "            else:\n",
    "                print(f'{key.capitalize()}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvImageClassifier(Classifier):\n",
    "    def __init__(self, input_dim, conv_layers, fc_layers, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stack = nn.Sequential(OrderedDict(\n",
    "            [\n",
    "                ('conv0', nn.Conv2d(in_channels=3, out_channels=conv_layers[0], kernel_size=3, padding=1)),\n",
    "                ('activation0', activation()),\n",
    "                ('maxpool0', nn.MaxPool2d(2)),\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "        for i in range(1, len(conv_layers)):\n",
    "            self.stack.add_module(f'conv{i}', nn.Conv2d(in_channels=conv_layers[i-1], out_channels=conv_layers[i], kernel_size=3, padding=1))\n",
    "            self.stack.add_module(f'activation{i}', activation())\n",
    "            self.stack.add_module(f'maxpool{i}', nn.MaxPool2d(2))\n",
    "            \n",
    "        conv_out = input_dim//(2**len(conv_layers))\n",
    "        self.stack.add_module('flatten', nn.Flatten())\n",
    "        self.stack.add_module(f'fc0', nn.Linear(conv_out**2*conv_layers[-1], fc_layers[0]))\n",
    "        \n",
    "        for i in range(1, len(fc_layers)):\n",
    "            self.stack.add_module(f'activation_fc{i}', nn.Tanh())\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(fc_layers[i-1], fc_layers[i]))        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "workers = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1a = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1a.stack)\n",
    "\n",
    "model_1a.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    train_len=len(cifar10_train),\n",
    "    test_loader=test_loader,\n",
    "    test_len=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 8e-3, 'weight_decay': 1e-2},\n",
    "    print_epoch=1,\n",
    "    header_epoch=15\n",
    ")\n",
    "\n",
    "del train_loader\n",
    "del test_loader\n",
    "del cifar10_train\n",
    "del cifar10_test\n",
    "\n",
    "model_1a.plot_training(\"2 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "workers = 16\n",
    "\n",
    "# Deal with loaders sticking around after interrupting training\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1b\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "model_1b = ConvImageClassifier(\n",
    "    input_dim = 32,\n",
    "    conv_layers=[32, 64, 128],\n",
    "    fc_layers=[32, 10],\n",
    "    activation=nn.ReLU\n",
    ").to(device=device)\n",
    " \n",
    "print(model_1b.stack)\n",
    "\n",
    "model_1b.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 3e-4, 'weight_decay': 1e-2}, #Increase alpha to 2 next time\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    print_epoch=1\n",
    ")\n",
    "del train_loader\n",
    "del test_loader\n",
    "model_1b.plot_training(\"3 Layer CNN Training Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after last bn but before last weight\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, nonlinearity = 'relu', stride=1, dropout = 0.4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.conv2 = nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False, stride=stride)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=out_chans)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.shortcut = nn.Conv2d(in_chans, out_chans, kernel_size=1, stride=1, bias=False) if in_chans != out_chans else nn.Identity()\n",
    "        \n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=nonlinearity)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "    def forward(self, x):\n",
    "        out = self.batch_norm1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        return out\n",
    "class ResNet(Classifier):\n",
    "    def __init__(self, input_dim = 32, n_blocks = 10, conv_channels = [32,16], fc_channels = [32, 10], dropout_p=0.4, dropout_h=0.4, nonlinearity='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add initial convolutions\n",
    "        self.h1 = nn.Sequential()\n",
    "        for i in range(len(conv_channels)):\n",
    "            self.h1.add_module(\n",
    "                name=f'conv{i}',\n",
    "                module=nn.Conv2d(\n",
    "                    in_channels = 3 if i == 0 else conv_channels[i-1],\n",
    "                    out_channels=conv_channels[i],\n",
    "                    kernel_size=3,\n",
    "                    padding=1\n",
    "                )\n",
    "            )\n",
    "            self.h1.add_module(\n",
    "                name=f'activation{i}',\n",
    "                module=nn.ReLU()\n",
    "            )\n",
    "        self.h1.add_module(\n",
    "            name=f'maxpool',\n",
    "            module=nn.MaxPool2d(2)\n",
    "        )\n",
    "        #output of h1 before maxpool is 16 32x32 images. After maxpool, 16 16x16 images\n",
    "# h1_in: torch.Size([1024, 3, 32, 32])\n",
    "# res_block_in: torch.Size([1024, 16, 16, 16])\n",
    "# h2_in: torch.Size([1024, 64])\n",
    "        #Add Resblocks\n",
    "        res_block_in = conv_channels[0]//2\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[\n",
    "                ResBlock(\n",
    "                    in_chans=conv_channels[-1],\n",
    "                    out_chans=conv_channels[-1],\n",
    "                    nonlinearity=nonlinearity,\n",
    "                    dropout=dropout_p\n",
    "                ) for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "        #output of resblocks is 16 16x16 images\n",
    "        \n",
    "        # Add final layers\n",
    "        self.h2 = nn.Sequential()\n",
    "        self.h2.add_module(\n",
    "            name='final_batch_norm',\n",
    "            module=nn.BatchNorm2d(\n",
    "                num_features=conv_channels[-1]\n",
    "            )\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name='final_relu',\n",
    "            module=nn.ReLU()\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'dropout_head',\n",
    "            module=nn.Dropout(dropout_h)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'gap',\n",
    "            module=nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.h2.add_module(\n",
    "            name = 'flatten',\n",
    "            module=nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #output is 16 8x8 images\n",
    "        #   16 comes from conv_channels[-1]\n",
    "        #   8x8 comes from input_dim // 4\n",
    "        fc_in = conv_channels[-1] * (input_dim//4)**2\n",
    "        \n",
    "        for i in range(len(fc_channels)):\n",
    "            \n",
    "            self.h2.add_module(\n",
    "                name=f'fc{i}',\n",
    "                module=nn.Linear(\n",
    "                    in_features=fc_in if i == 0 else fc_channels[i-1],\n",
    "                    out_features=fc_channels[i]\n",
    "                )\n",
    "            )\n",
    "            if i < len(fc_channels) - 1:\n",
    "                self.h2.add_module(\n",
    "                    name = f'fc_activation{i}',\n",
    "                    module=nn.ReLU()\n",
    "                )\n",
    "        self.h2.add_module('softmax', nn.Softmax(dim=1))\n",
    "    def forward(self, x):\n",
    "        #print(f\"h1_in: {x.shape}\")\n",
    "        out = self.h1(x)\n",
    "        #print(f\"res_block_in: {out.shape}\")\n",
    "        out = self.resblocks(out)\n",
    "        #print(f\"h2_in: {out.shape}\")\n",
    "        out = self.h2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin init train_loader\n",
      "Batch Size: 12.0 MiB\n",
      "begin init fetcher\n",
      "Init time: 3.63 seconds\n",
      "Training ResNet\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   D Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         0          |      6.815956      |      2.301220      |      2.301946      |      0.031548      |      0.100200      |      0.000000      |      2.965976      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         1          |      5.230381      |      2.293843      |      2.294098      |      0.011121      |      0.139200      |     38.922161      |      3.272617      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         2          |      5.257645      |      2.281738      |      2.284173      |      0.106705      |      0.149700      |      7.543103      |      3.579258      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         3          |      5.246727      |      2.272303      |      2.277372      |      0.223089      |      0.159000      |      6.212420      |      3.885899      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         4          |      5.389300      |      2.263654      |      2.269428      |      0.255086      |      0.171300      |      7.735849      |      4.194493      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         5          |      5.147517      |      2.256124      |      2.260385      |      0.188886      |      0.183600      |      7.180386      |      4.501133      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         6          |      5.206008      |      2.247957      |      2.253533      |      0.248075      |      0.201000      |      9.477122      |      4.807774      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         7          |      5.131864      |      2.236525      |      2.245339      |      0.394130      |      0.234000      |     16.417915      |      5.114415      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         8          |      5.203375      |      2.217948      |      2.223889      |      0.267856      |      0.243900      |      4.230771      |      5.421055      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         9          |      5.251090      |      2.197910      |      2.197752      |     -0.007159      |      0.264600      |      8.487076      |      5.727696      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         10         |      5.275305      |      2.181126      |      2.184466      |      0.153154      |      0.275300      |      4.043847      |      6.034337      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         11         |      5.326387      |      2.169039      |      2.179607      |      0.487194      |      0.280100      |      1.743550      |      6.340977      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         12         |      5.210192      |      2.160122      |      2.169880      |      0.451734      |      0.290600      |      3.748666      |      6.647618      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         13         |      5.261581      |      2.151361      |      2.162727      |      0.528301      |      0.301600      |      3.785264      |      6.954258      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         14         |      5.260305      |      2.144923      |      2.150099      |      0.241339      |      0.313700      |      4.011940      |      7.260899      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   D Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         15         |      5.374393      |      2.138357      |      2.148208      |      0.460702      |      0.314200      |      0.159386      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         16         |      5.311073      |      2.130789      |      2.145388      |      0.685149      |      0.319300      |      1.623174      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         17         |      5.229197      |      2.122706      |      2.126566      |      0.181866      |      0.336200      |      5.292829      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         18         |      5.206430      |      2.114970      |      2.121946      |      0.329811      |      0.348600      |      3.688281      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         19         |      5.200324      |      2.107492      |      2.110137      |      0.125517      |      0.358000      |      2.696495      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         20         |      5.298649      |      2.099614      |      2.104376      |      0.226778      |      0.361200      |      0.893862      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         21         |      5.181546      |      2.092328      |      2.096375      |      0.193383      |      0.369000      |      2.159464      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         22         |      5.343644      |      2.085334      |      2.092034      |      0.321282      |      0.373300      |      1.165311      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         23         |      5.237906      |      2.078858      |      2.092339      |      0.648453      |      0.374300      |      0.267886      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         24         |      5.227514      |      2.072461      |      2.072086      |     -0.018119      |      0.395100      |      5.557038      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         25         |      5.265506      |      2.066865      |      2.070712      |      0.186122      |      0.396100      |      0.253097      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         26         |      5.235468      |      2.062755      |      2.072860      |      0.489920      |      0.397900      |      0.454431      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         27         |      5.332780      |      2.058389      |      2.062650      |      0.206984      |      0.407200      |      2.337269      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         28         |      5.274902      |      2.053660      |      2.062477      |      0.429306      |      0.407600      |      0.098233      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         29         |      5.190897      |      2.048340      |      2.060210      |      0.579536      |      0.409200      |      0.392541      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   D Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         30         |      5.257972      |      2.044727      |      2.048901      |      0.204146      |      0.423000      |      3.372433      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         31         |      5.185754      |      2.039118      |      2.039599      |      0.023595      |      0.430700      |      1.820330      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         32         |      5.223942      |      2.035442      |      2.042032      |      0.323781      |      0.428100      |     -0.603665      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         33         |      5.212322      |      2.031315      |      2.039691      |      0.412326      |      0.428800      |     -0.441139      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         34         |      5.190100      |      2.026623      |      2.029923      |      0.162842      |      0.440600      |      2.298585      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         35         |      5.216954      |      2.022188      |      2.030001      |      0.386327      |      0.437300      |     -0.748974      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         36         |      5.200002      |      2.019151      |      2.025975      |      0.338000      |      0.445000      |      0.998642      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         37         |      5.222679      |      2.014743      |      2.023756      |      0.447349      |      0.445900      |      0.202247      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         38         |      5.152236      |      2.011357      |      2.017721      |      0.316409      |      0.449800      |      0.874634      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         39         |      5.237659      |      2.007771      |      2.015530      |      0.386417      |      0.449600      |     -0.044465      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         40         |      5.264621      |      2.004523      |      2.007381      |      0.142598      |      0.462200      |      2.756781      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         41         |      5.266026      |      2.000401      |      2.003386      |      0.149184      |      0.464100      |      0.411075      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         42         |      5.286573      |      1.997357      |      2.007950      |      0.530348      |      0.459300      |     -1.034258      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         43         |      5.186550      |      1.995211      |      1.997260      |      0.102700      |      0.469300      |      1.120454      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         44         |      5.266215      |      1.989504      |      1.996932      |      0.373362      |      0.470600      |      0.277003      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   D Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         45         |      5.284018      |      1.986926      |      1.995589      |      0.436003      |      0.472200      |      0.339991      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         46         |      5.301389      |      1.984263      |      1.988132      |      0.194993      |      0.481400      |      1.948328      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         47         |      5.309591      |      1.980890      |      1.987547      |      0.336037      |      0.481000      |     -0.083092      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         48         |      5.339252      |      1.976941      |      1.981631      |      0.237244      |      0.487100      |      1.184045      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         49         |      5.223598      |      1.975473      |      1.984607      |      0.462385      |      0.482200      |     -1.005949      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         50         |      5.505944      |      1.972248      |      1.981295      |      0.458705      |      0.486100      |     -0.205294      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         51         |      5.331898      |      1.971247      |      1.972440      |      0.060498      |      0.493200      |      1.252309      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         52         |      5.203537      |      1.968757      |      1.971757      |      0.152381      |      0.493600      |      0.081104      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         53         |      5.204344      |      1.966531      |      1.975052      |      0.433299      |      0.491500      |     -0.425444      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         54         |      5.441829      |      1.964058      |      1.965333      |      0.064914      |      0.500700      |      1.438415      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         55         |      5.269188      |      1.961235      |      1.965907      |      0.238238      |      0.499400      |     -0.259638      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         56         |      5.280013      |      1.957596      |      1.956633      |     -0.049186      |      0.508200      |      1.497902      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         57         |      5.260606      |      1.957089      |      1.955194      |     -0.096843      |      0.511900      |      0.728063      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         58         |      5.209586      |      1.955901      |      1.957924      |      0.103436      |      0.506700      |     -1.015829      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         59         |      5.257082      |      1.951796      |      1.955865      |      0.208473      |      0.507400      |     -0.879084      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   D Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         60         |      5.215636      |      1.951298      |      1.954270      |      0.152279      |      0.510000      |     -0.371170      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         61         |      5.240692      |      1.948700      |      1.944321      |     -0.224710      |      0.520700      |      1.719080      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         62         |      5.225616      |      1.947137      |      1.947294      |      0.008100      |      0.515000      |     -1.094679      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         63         |      5.333434      |      1.943967      |      1.945327      |      0.069926      |      0.518400      |     -0.441706      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         64         |      5.191917      |      1.943048      |      1.944095      |      0.053934      |      0.521000      |      0.057613      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         65         |      5.145137      |      1.940725      |      1.939351      |     -0.070774      |      0.524400      |      0.652597      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         66         |      5.172527      |      1.941706      |      1.941124      |     -0.029997      |      0.524700      |      0.057206      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         67         |      5.171399      |      1.937627      |      1.932986      |     -0.239560      |      0.531700      |      1.334101      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         68         |      5.099522      |      1.936373      |      1.934624      |     -0.090301      |      0.529600      |     -0.394969      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         69         |      5.167198      |      1.935155      |      1.936521      |      0.070614      |      0.529100      |     -0.489000      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         70         |      5.299881      |      1.934560      |      1.935916      |      0.070057      |      0.527500      |     -0.789927      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         71         |      5.183186      |      1.932120      |      1.929466      |     -0.137366      |      0.534400      |      0.507800      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         72         |      5.197831      |      1.931704      |      1.929420      |     -0.118265      |      0.535600      |      0.224555      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         73         |      5.238177      |      1.929137      |      1.925259      |     -0.201035      |      0.538100      |      0.466766      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         74         |      5.198441      |      1.928382      |      1.926552      |     -0.094860      |      0.535900      |     -0.408847      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   D Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         75         |      5.302837      |      1.926299      |      1.928862      |      0.133065      |      0.533100      |     -0.929194      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         76         |      5.150648      |      1.925350      |      1.921428      |     -0.203721      |      0.540300      |      0.408847      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         77         |      5.152917      |      1.926890      |      1.930431      |      0.183773      |      0.534300      |     -1.110502      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         78         |      5.129626      |      1.923733      |      1.924192      |      0.023845      |      0.537300      |     -0.555251      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         79         |      5.229180      |      1.922218      |      1.919101      |     -0.162161      |      0.544200      |      0.721820      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         80         |      5.234832      |      1.921953      |      1.916134      |     -0.302763      |      0.545600      |      0.257257      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         81         |      5.365631      |      1.920037      |      1.916518      |     -0.183250      |      0.547700      |      0.384896      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         82         |      5.202475      |      1.919796      |      1.916709      |     -0.160782      |      0.544000      |     -0.675556      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         83         |      5.298961      |      1.918155      |      1.917854      |     -0.015674      |      0.544900      |     -0.511227      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         84         |      5.304288      |      1.916769      |      1.913228      |     -0.184738      |      0.548400      |      0.127807      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         85         |      5.323539      |      1.918119      |      1.913656      |     -0.232649      |      0.548400      |      0.000000      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         86         |      5.425175      |      1.915725      |      1.911342      |     -0.228783      |      0.550500      |      0.382931      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         87         |      5.127525      |      1.914026      |      1.910430      |     -0.187861      |      0.549700      |     -0.145325      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         88         |      5.191898      |      1.914132      |      1.910165      |     -0.207232      |      0.554300      |      0.690288      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         89         |      5.152778      |      1.911341      |      1.909087      |     -0.117966      |      0.554300      |      0.000000      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   D Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         90         |      5.176626      |      1.911426      |      1.905907      |     -0.288708      |      0.554700      |      0.072154      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         91         |      5.145042      |      1.910549      |      1.909623      |     -0.048462      |      0.553100      |     -0.288438      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         92         |      5.151623      |      1.910203      |      1.907595      |     -0.136527      |      0.552300      |     -0.432663      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         93         |      5.178242      |      1.909798      |      1.905341      |     -0.233357      |      0.558000      |      0.594918      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         94         |      5.152971      |      1.909071      |      1.910553      |      0.077655      |      0.551000      |     -1.254475      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         95         |      5.121346      |      1.908431      |      1.906575      |     -0.097276      |      0.556500      |     -0.268819      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         96         |      5.220850      |      1.905869      |      1.903353      |     -0.132040      |      0.559300      |      0.232982      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         97         |      5.207913      |      1.906565      |      1.900703      |     -0.307458      |      0.561000      |      0.303949      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         98         |      5.218857      |      1.907416      |      1.905560      |     -0.097328      |      0.557000      |     -0.713014      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|         99         |      5.132508      |      1.904255      |      1.902587      |     -0.087630      |      0.559800      |     -0.213907      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        100         |      5.221685      |      1.905596      |      1.900390      |     -0.273207      |      0.562100      |      0.196079      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        101         |      5.127095      |      1.903296      |      1.900043      |     -0.170957      |      0.562500      |      0.071163      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        102         |      5.217600      |      1.902551      |      1.899179      |     -0.177240      |      0.562600      |      0.017770      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        103         |      5.217314      |      1.902714      |      1.900743      |     -0.103614      |      0.561000      |     -0.284388      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        104         |      5.186641      |      1.903621      |      1.896755      |     -0.360674      |      0.563900      |      0.231077      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|       Epoch        |   Epoch Time (s)   |   Training Loss    |     Test Loss      |    Overfit (%)     |    Accuracy (%)    |   D Accuracy (%)   |  GPU Memory (GiB)  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        105         |      5.172439      |      1.899969      |      1.894399      |     -0.293184      |      0.567900      |      0.709347      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        106         |      5.180629      |      1.900323      |      1.895101      |     -0.274831      |      0.566400      |     -0.264133      |      7.567540      |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 65\u001b[0m\n\u001b[1;32m     56\u001b[0m resnet \u001b[38;5;241m=\u001b[39m ResNet(\n\u001b[1;32m     57\u001b[0m     input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     58\u001b[0m     conv_channels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m8\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     dropout_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n\u001b[1;32m     63\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInit time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[43mresnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcifar10_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m resnet\u001b[38;5;241m.\u001b[39mplot_training(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet Training Curves\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#[m1,n1]x[m2,n2] is only possible if n1 == m2\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 82\u001b[0m, in \u001b[0;36mClassifier.train_model\u001b[0;34m(self, epochs, train_loader, test_loader, train_len, test_len, test_size, loss_fn, optimizer, optimizer_args, optimizer_kwargs, print_epoch, header_epoch)\u001b[0m\n\u001b[1;32m     80\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(Y_pred, Y_batch)\n\u001b[1;32m     81\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     85\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model_1a\n",
    "    del model_1b\n",
    "    del resnet\n",
    "    del train_loader_cuda\n",
    "    del test_loader_cuda\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reset CUDA context\n",
    "start = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=dl, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=dl, transform=transform)\n",
    "\n",
    "batch_size = int(2**10)\n",
    "workers = 12\n",
    "cpu_prefetch = 18\n",
    "gpu_prefetch = 14\n",
    "\n",
    "print('begin init train_loader')\n",
    "train_loader = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    prefetch_factor=cpu_prefetch,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "X_batch = next(iter(train_loader))[0]\n",
    "dtype_size = X_batch.element_size()\n",
    "print(f\"Batch Size: {X_batch.element_size() * X_batch.nelement() / 1024**2} MiB\")\n",
    "\n",
    "\n",
    "print('begin init fetcher')\n",
    "train_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = train_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=gpu_prefetch\n",
    ")\n",
    "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=True, num_workers=workers, pin_memory=True, prefetch_factor=1)\n",
    "test_loader_cuda = CudaDataPrefetcher(\n",
    "    data_iterable = test_loader,\n",
    "    device = torch.device('cuda'),\n",
    "    num_prefetch_batches=1\n",
    ")\n",
    "\n",
    "resnet = ResNet(\n",
    "    input_dim = 32,\n",
    "    conv_channels=[16,8],\n",
    "    n_blocks = 10,\n",
    "    fc_channels=[16,10],\n",
    "    dropout_h = 0.6,\n",
    "    dropout_p = 0.4\n",
    ").to(device=device)\n",
    "print(f\"Init time: {(time.time() - start):.2f} seconds\")\n",
    "resnet.train_model(\n",
    "    epochs=200,\n",
    "    train_loader=train_loader_cuda,\n",
    "    train_len=len(train_loader),\n",
    "    test_loader=test_loader_cuda,\n",
    "    test_len=len(test_loader),\n",
    "    test_size=len(cifar10_test),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 1e-4, 'weight_decay': 1e-4},\n",
    "    print_epoch=1\n",
    ")\n",
    "resnet.plot_training(\"ResNet Training Curves\")\n",
    "\n",
    "#[m1,n1]x[m2,n2] is only possible if n1 == m2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
